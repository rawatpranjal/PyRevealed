--- Page 1 ---
Empirical Revealed Preference
Ian Crawford
University of Oxford & Nuﬃeld College


--- Page 2 ---
Overview
”There is nothing more practical then a good theory” - L. Boltzmann
• An elementary, “structural” approach to the analysis and interpreta-
tion of data by means of economic theory.
• It is somewhat distinct from structural econometrics because
– it avoids having to resort to error terms
– minimises the use of untestable assumptions
– is expressed in terms of empirical inequalities.


--- Page 3 ---
Overview
• Typically we use economic theory to develop formal statements con-
cerning causes and eﬀects.
– causes (explanatory variables) which may be observed (x) or un-
observed (η)
– eﬀects (endogenous variables, y)
• These are linked by structural equations which are theory-driven
y = f(x, η, θ)
where θ represents a set of unknown parameters or functions.


--- Page 4 ---
Overview
Econometricians always then append a statistical structure to the economic
model in order to account for the fact that the model does not perfectly
explain the data.
“The ... interpretation is that the true utility used by consumers to
make choices is deterministic, but due to the researcher’s inability
to formulate individual behavior precisely, an additional stochastic
term is added, thus making utility stochastic from the researcher’s
point of view (see Manski 1977; McFadden 1981, 1984). This is
the interpretation followed in the economics literature” - Nevo
(Annual Reviews of Economics, 2011, p. 59)


--- Page 5 ---
Overview
This relatively recent, authoritative survey echoes identical views expressed
nearly 70 years earlier by Haavelmo:
“Observable economic variables do not satisfy exact relationships
(except, perhaps, some trivial identities). Therefore, if we start
out with such a theoretical scheme, we have - for the purpose of
application - to add some stochastical elements, to bridge the gap
between the theory and the facts.” - Haavelmo, (Econometrica,
1944.)


--- Page 6 ---
Overview
• This extra structure entails
– the introduction of unobservable econometric error terms (ε)
– statistical assumptions regarding the joint distribution of (x, η, ε)
• When combined these economic and statistical assumptions deliver an
empirical model that is capable of rationalising any set of observables.


--- Page 7 ---
Overview
• The art of structural econometric modelling thus mainly lies in getting
this statistical aspect right, because the source and the properties of
these econometric errors ε can have a critical impact on the estimation
results.
• This can be a challenge because:
– economic theories, which are by and large completely deterministic,
generally have little to say about the statistical model,
– the data have generally little to say about the unobserved (η) and
unobservable (ε).


--- Page 8 ---
Overview
• Empirical revealed preference also begins from economic theory, is
entirely diﬀerent to the “y = f(x, η, θ, ε)” type of framework.
• Uses systems of inequalities which depend neither on the form of struc-
tural functions nor on unobservables.
• Statistical error terms and speciﬁc assumptions about the functional
structure of the economic model may be added but it is not an essential
requirement.
• In a sense empirical revealed preference is concerned with what we can
learn simply by combining economic theory with the features of the
world that we can observe.


--- Page 9 ---
Consumer Theory
max
q
u (q) subject to p′tq ≤xt
• Suppose we observe some data on prices and choices {pt, qt}t=1,...,T
for an individual consumer.
– If the data were generated by the model, what properties must they
necessarily have?
– If the observed data have these properties, is that suﬃcient to
know that they could have been generated by the model?
• What are the necessary and suﬃcient conditions for this model?


--- Page 10 ---
Necessity and Suﬃciency
• Necessity and suﬃciency are implicational relations between state-
ments.
• Necessity: If A ⇒B then B is a necessary condition for A.
– If A is true B then is necessarily true.
– In particular A ⇒B is equivalent to ¬B ⇒¬ A
• Suﬃciency: If A ⇐B then B is a suﬃcient condition for A.
– If B is true then that is suﬃcient to know that A is true too.
– In particular A ⇐B is equivalent to ¬A ⇒¬ B


--- Page 11 ---
Necessity and Suﬃciency
• The assertion that one statement is necessary and suﬃcient for another
means that the former statement is true if and only if the latter is true.
A ⇔B
• That is, the two statements are
– equivalent
– simultaneously true or simultaneously false.
A ⇒B, ¬B ⇒¬ A, B ⇒A, ¬A ⇒¬ B


--- Page 12 ---
Necessity and Suﬃciency
max
q
u (q) subject to p′tq ≤xt
• Suppose we observe some data on prices and choices {pt, qt}t=1,...,T
for an individual consumer.
– If the data were generated by the model what properties must the
data necessarily have?
– If we observe these properties in some data, is that suﬃcient to
know that the data could have been generated by the model?


--- Page 13 ---
Afriat’s Theorem
We are interested in whether there is agreement between theory and data.
We ﬁrst need to deﬁne what that means.
Deﬁnition: A utility function u (q) rationalises the data {pt, qt}t=1,...,T if
u (qt) ≥u (q) for all q such that p′tqt ≥p′tq.


--- Page 14 ---
Afriat’s Theorem∗
The following statements are equivalent:
A. there exists a utility function u (q) which is continuous, non-
satiated and concave which rationalises the data {pt, qt}t=1,...,T.
B1. there exist numbers {Ut, λt > 0}t=1,...,T such that
Us ≤Ut + λtp′t (qs −qt) ∀s, t ∈{1, ..., T}
B2.
the data {pt, qt}t=1,...,T satisfy the Generalised Axiom of
Revealed Preference (GARP).
C. there exists a non-satiated utility function u (q) which ratio-
nalises the data {pt, qt}t=1,...,T.
∗Afriat (1967), Diewert (1973), Varian (1982).


--- Page 15 ---
Afriat’s Theorem
• We are going to take A to be true and work out why it implies the
condition B1.
• By concavity of u (q) we have
u (qs) ≤u (qt) + ∇u (qt)′ (qs −qt)
• Optimising behaviour implies the ﬁrst-order condition ∇u (qt) ≤λtpt
where λt > 0 and with equality when qkt > 0.


--- Page 16 ---
Afriat’s Theorem
• Putting the foc into the concavity condition preserves the inequality
and gives
u (qs) ≤u (qt) + λtp′t (qs −qt)
• Utility functions are real-valued so there must therefore exist real num-
bers {Ut, λt > 0}t=1,...,T corresponding to the values of u (qt) and λt
Us ≤Ut + λtp′t (qs −qt) ∀s, t ∈{1, ..., T}
• This is condition B1.


--- Page 17 ---
Afriat’s Theorem
• We are going to take B1 to be true (i.e. that we observe some data for
which this condition holds) and work out why it implies the condition
A.
• The path we are going to take is constructive: we are going to build
a utility function out of the available raw materials and show that it
does indeed rationalise the data.
• The raw materials are a set of {Ut, λt > 0}t=1,...,T which satisfy the
inequalities
Us ≤Ut + λtp′t (qs −qt) ∀s, t ∈{1, ..., T}


--- Page 18 ---
Afriat’s Theorem
• Let
u (q) =
min
s∈{1,..,T}
n
Us + λsp′s (q −qs)
o
s=1,...,T
be our utility function (it’s piecewise linear, continuous, non-satiated,
and concave).
• We now need to show that this rationalises the data.
• That means that u (qt) ≥u (q) for all q such that p′tqt ≥p′tq.


--- Page 19 ---
Afriat’s Theorem
• Suppose we have some q with p′tqt ≥p′tq. We need to show that
∃some u(q) such that u(qt) ≥u(q)
• Firstly consider the observation qt.
• What utility number does our function associate with it?
u (qt) =
min
s∈{1,..,T}
n
Us + λsp′s (qt −qs)
o


--- Page 20 ---
Afriat’s Theorem
• One element of the set concerns the case where t = s in which case
the corresponding element is:
Ut + λtp′t (qt −qt) = Ut
• We know that Ut ≤Us + λsp′s (qt −qs) ∀s, t
• Therefore Ut ≤mins∈{1,...,T}
Us + λsp′s (qt −qs)
	
• So our utility function assigns
u (qt) = Ut


--- Page 21 ---
Afriat’s Theorem
• Now consider the aﬀordable alternative q. What utility number does
our function associate with it?
u (q) =
min
s∈{1,..,T}
n
Us + λsp′s (q −qs)
o
• We know that
u (q) =
min
s∈{1,..,T}
n
Us + λsp′s (q −qs)
o
≤Ut + λtp′t (q −qt)


--- Page 22 ---
Afriat’s Theorem
• Finally λt > 0 and p′tqt ≥p′tq means λtp′t (q −qt) ≤0. So u (q) ≤
Ut
• Therefore we have shown that for any q with p′tqt ≥p′tq
u(q) ≤u(qt)
• Hence that our utility function rationalises the data in the required
sense.


--- Page 23 ---
Afriat’s Theorem
• Afriat’s Theorem presents necessary and suﬃcient conditions for the
standard utility-maximisation model.
• It shows that if a dataset can be rationalised by any utility function
then it can in fact be rationalised by a well-behaved one - with com-
petitive pricing non-convexities are ”shrouded in enternal darkness”
• It is therefore exhaustive: it summarises ALL of the empirical impli-
cations which come from the basic model without making any special
assumptions on functional forms (other than those needed for well-
behavedness).


--- Page 24 ---
Afriat’s Theorem
Deﬁnition: Given an observation qt and a bundle q:
(i) qt is directly revealed preferred to q, written qtR0q
if p′tqt ≥p′tq
(ii) qt is strictly directly revealed preferred to q, written qtP0q if
p′tqt ≥p′tq ;
(iii) qt is revealed preferred to q, written qtRq if p′tqt ≥p′tqu,
p′uqu ≥p′uqv, ..., p′vqv ≥p′vq for some sequence of observations
qt, qu, qv, ..., q,. In this case we say that the relation R is the
transitive closure of the relation R0.
(iv) qt is strictly revealed preferred to q, written qtPq , if there
exist observations qi and qj such that qtRqi, qiP0qj, qjRq.


--- Page 25 ---
Afriat’s Theorem
GARP: qtRqs implies NOT qsP0qt
If a consumption bundle qt is revealed preferred to a consumption bundle
qs, then qs cannot be strictly directly revealed preferred to qt.


--- Page 26 ---
Afriat’s Theorem
Suppose that we have four observations {pt, qt}t=1,...,4 and that
p′
1q1 ≥p′
1q3
: q1R0q3
p′
2q2 > p′
2q1
: q2P0q1 (also q2R0q1)
p′
2q2 ≥p′
2q4
: q2R0q4
p′
3q3 ≥p′
3q2
: q3R0q2


--- Page 27 ---
Afriat’s Theorem
We can write this into a matrix m where mst = 1 if qsR0qt and zero
otherwise:


1
0
1
0
1
1
0
1
0
1
1
0
0
0
0
1


In graph theory and computer science a square matrix made of zeros and
ones is used to represent a simple ﬁnite directed graph.
It’s called an
“adjacency matrix”. The rows and columns label the graph vertices, with
a 1 or 0 in (row, col) (s, t) according to whether s and t are adjacent.


--- Page 28 ---
Afriat’s Theorem
In a “directed graph” the adjacency is directional, there is an edge from s
to t connecting them.
A directed graph has a cycle if it is possible to walk from any vertex and
follow a consistently-directed sequence of edges that eventually loops back
to that same vertex again.
GARP is a kind of “no-cyclic” condition on the directed graph generated
by the data.


--- Page 29 ---
Afriat’s Theorem
If we look at our data we can plot the directed graph for the R0 relations:
There is clearly a cycle starting/ending at q1:
q1R0q3, q3R0q2, q2R0q1


--- Page 30 ---
The potential problem (for economics) with this is that the ﬁrst two steps
imply that this consumer prefers q1 to q2 (albeit indirectly via transitivity)
so
q1Rq2
whilst we have the direct relation
q2R0q1
This is only a “potential problem”: it is OK if and only if none of the
revealed preferences are strict - because they may be indiﬀerent between
q1 and q2. But if there is a strict preference for q2 over q1 then GARP is
violated and the individual’s preferences are not representable by a utility
function.


--- Page 31 ---
Afriat’s Theorem
If we look at our data we can indicate the edges for the P0 relation:
Now we can see that q1 is revealed preferred to q2 and that q2 is directly
strictly revealed preferred to q1. Another way of putting it is that q1 is
revealed strictly preferred to itself.


--- Page 32 ---
Afriat’s Theorem
• The Strong Axiom of Revealed Preference,
qtRqs and qt ̸= qs implies NOT qsR0qt
• SARP implies GARP, but not vice versa.
• SARP requires single valued demand functions while GARP is com-
patible with multivalued demand functions (correspondences).


--- Page 33 ---
Afriat’s Theorem
• The Weak Axiom of Revealed Preference
qtR0qs and qt ̸= qs implies NOT qsR0qt
• Does not involve transitivity.
• Is necessary and suﬃcient for utility maximisation when there are only
two goods - why?


--- Page 34 ---
Afriat’s Theorem - Some history
• The theory of revealed preference was ﬁrst introduced by Paul Samuel-
son in his 1938 Economica article.
• Samuelson’s view was that economics was really about derivation of
”meaningful theorems”
”By a meaningful theorem I mean simply a hypothesis about empirical
data which could conceivably be refuted.”
P. Samuelson, Foundations of Economic Analysis, p.4, (1947).


--- Page 35 ---
Afriat’s Theorem - Some history
• His aim was to derive testable implications of theory without ﬁrst pos-
tulating a utility function that represents the consumer’s preferences.
• He argued that the testable implications of the theory should be based
on axioms about observable demands rather than on axioms about
unobservable preferences.
• The focus on the observable rather than the unobservable remains at
the heart of the topic.


--- Page 36 ---
Afriat’s Theorem - Some history
• Houthakker (1950) extended Samuelson’s work by introducing the
Strong Axiom of Revealed Preference (SARP).
• SARP works for any number of budget sets and works by exploiting
transitivity.
• He also demonstrated that demand functions satisfy SARP if and only
if they are the result of the maximisation of well-behaved preferences
subject to the consumer’s budget constraint.
• Clearly, this establishes a close link, also recognised by Samuelson,
between the axioms about demand and the axioms about preferences.


--- Page 37 ---
Afriat’s Theorem - Some history
• But both Samuelson and Houthakker assumed that the researcher
could observe the entire demand system.
• If you could observe the entire demand system then the question of
testable implications could as easily be addressed using the ”standard”
diﬀerential approach which goes back to Slutsky (1915) and Antonelli
(1886).
• But we do not observe the entire demand system.
• We only ever observed a ﬁnite number of observations.


--- Page 38 ---
Afriat’s Theorem - Some history
• The structural econometric approach makes up for this “data deﬁcit”
by ﬁtting functions to the (ﬁnite) data.
• These functions are like having an inﬁnite amount of data - once
estimated we can evaluate them anywhere/everywhere and check the
integrability conditions.
• Of course this is easier said than done.


--- Page 39 ---
Afriat’s Theorem - Some history
• In particular, in order to estimate them consistently, requires us to
make untestable auxiliary statistical assumptions.
• Any test of the hypothesis of maximising behaviour therefore is really
a test of a joint hypothesis: the behaviour of interest plus the auxiliary
statistical hypotheses required to deliver the estimate.
• This is the essence of the Duhem-Quine problem in the philosophy of
science.


--- Page 40 ---
Afriat’s Theorem - Some history
• Afriat in his 1967 International Economic Review article focussed on,
and solved, the same problem but with only a ﬁnite number of obser-
vations.
• This might seems a small thing but it was the key to liberating applied
work from the need to rely on assumed properties of unobserved and
unobservable quantities.
• In that sense it represents the fruition of Samuelson’s quest for a truly
Meaningful Theorem.


--- Page 41 ---
Afriat’s Theorem - Some history
• There was a very important further contribution by Erwin Diewert in
the Review of Economic Studies in 1973.
• He analysed which assumptions on the utility function must be satisﬁed
so that a solution to the utility maximisation problem exists in the ﬁrst
place. This was ignored in Afriat (1967).
• It turned out that the assumption of local non-satiation is crucial in
this respect. Without local non-satiation, it may be the case that there
is no solution to the utility maximisation problem.


--- Page 42 ---
Afriat’s Theorem - Some history
• Moreover, without local non-satiation, any set of observed choices can
be rationalised by a utility function in a trivial way - by resorting to
”thick” indiﬀerence curves.
• Diewert (1973) also demonstrated that a linear programme can be
constructed to solve the testability and recoverability questions.
• This was the ﬁrst step on the way to translating Afriat’s rather im-
penetrable work into something which could actually be applied.


--- Page 43 ---
Afriat’s Theorem - Some history
• Varian’s contributions begins with his 1982 Econometrica article.
• He solved or simpliﬁed many of the most important computational
aspects of revealed preference
• He also extended Afriat’s and Diewert’s work by considering the re-
coverability and extrapolation questions.


--- Page 44 ---
Applying Afriat’s Theorem
• Applied consumer theory typically addresses three sorts of issues:
1. Consistency. When is observed behaviour consistent with the model?
2. Recoverability/The inverse problem. How can we recover preferences
given observations on consumer behavior?
3. Extrapolation/The forward problem.
Given consumer behaviour for
some budgets how can we forecast behaviour for other budgets?


--- Page 45 ---
Applying Afriat’s Theorem
• Doing applied work using RP restrictions requires a completely dif-
ferent set of techniques than does the “y = f(x, η, θ, ε)” type of
framework.
• The methods are mainly algorithmic or combinatorial - more typically
learned when studying operations research than economics.


--- Page 46 ---
Applying Afriat’s Theorem
• Given some data {pt, qt}t=1,...,T to check for consistency with the
theory we can either
1. Determine whether there exist numbers {Ut, λt > 0}t=1,...,T such that
Us ≤Ut + λtp′t (qs −qt) ∀s, t ∈{1, ..., T}
2. Determine whether the data satisfy GARP.


--- Page 47 ---
Applying Afriat’s Theorem
• Suppose we have just two observations {p1, p2; q1, q2}.
• Then the Afriat Inequalities
Us ≤Ut + λtp′t (qs −qt)
and λt > 0, ∀s, t ∈{1, 2}
are (explicitly)
U1 −U1 −λ1p′
1 (q1 −q1)
≤
0
U1 −U2 −λ2p′
2 (q1 −q2)
≤
0
U2 −U1 −λ1p′
1 (q2 −q1)
≤
0
U2 −U2 −λ2p′
2 (q2 −q2)
≤
0
−λ1
<
0
−λ2
<
0


--- Page 48 ---
Applying Afriat’s Theorem
• This can be written, more compactly, as


0
0
0
0
1
−1
0
−p′
2 (q1 −q2)
−1
1
−p′
1 (q2 −q1)
0
0
0
0
0
0
0
−1
0
0
0
0
−1




U1
U2
λ1
λ2

≤


0
0
0
0
−ε
−ε


where the ε represent arbitrarily small constants.


--- Page 49 ---
Applying Afriat’s Theorem
• The check for consistency is therefore whether (or not) there exists a
vector x such that
Ax ≤b
• In essence we are asking whether there exist a solution to a set of
linear inequalities
• This is a linear programming problem. Linear programs are problems
that can be expressed in the general form:
max
x
c′x subject to Ax ≤b


--- Page 50 ---
Applying Afriat’s Theorem
• The ”simplex algorithm”, developed by George Dantzig in 1947, solves
LP problems by
1. constructing a feasible solution iﬀa solution exists, and then
2. optimising it.
• We are only concerned with Phase 1.
• Dantzig’s algorithm can determine whether or not there is a feasible
solution in a ﬁnite number of steps (a trial and error approach would,
conversely, never be guaranteed to stop).


--- Page 51 ---
Applying Afriat’s Theorem
• In general checking for consistency requires a linear program with 2T
variables and T 2 constraints.
• The fact that the number of constraints rises as the square of the
number of observations can makes this condition computationally de-
manding in practice for very large datasets.
• Condition B2 (GARP) is sometimes more eﬃcient.
• This requires us to compute the transitive closure of a ﬁnite relation
but that is certainly a ﬁnite problem and Warshall (1962) gives a
solution in T 3 steps. It is very easy to implement.


--- Page 52 ---
Applying Afriat’s Theorem
GARP provides another way to test for utility maximisation.
1. Loop through the data and create the directed adjacency matrix m
for the R0 relation
2. Compute the transitive closure of the graph (Floyd-Warshall algorithm)
to ﬁll in the implied R relations.
3. Loop through the data and create adjacency matrix n for the P0 rela-
tions
4. Add the two matrices and check there are no “2’s”.


--- Page 53 ---
Applying Afriat’s Theorem
In the example we had when showing the graph interpretation of GARP
we had the R0 graph:
m =


1
0
1
0
1
1
0
1
0
1
1
0
0
0
0
1


The transitive closure is
m =


1 1
1
0
1
1
0
1
0
1
1
0
0
0
0
1




--- Page 54 ---
Applying Afriat’s Theorem
In our example the P0 graph was:
n =


0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0


Adding data the P0 graph gives:
m + n =


1 1
1
0
1
1
0
1
0
1
1
0
0
0
0
1

+


0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0

=


1
1
1
0
2
1
0
1
0
1
1
0
0
0
0
1




--- Page 55 ---
Applying Afriat’s Theorem
The data used here is the Spanish Continuous Family Expenditure Survey
(the Encuesta Continua de Presupuestos Familiares - ECPF).
The ECPF is a quarterly budget survey of Spanish households which inter-
views about 3,200 households every quarter.
These households are randomly rotated at a rate of 12.5% each quarter.
It is possible to follow a participating household for up to eight consecutive
quarters.
It’s nationally representative and its coverage of expenditure is wide.


--- Page 56 ---
Applying Afriat’s Theorem
The data from the period 1985 to 1997 and are the selected sub-sample
of couples with and without children, in which the husband is in full-time
employment in a non-agricultural activity and the wife is out of the labour
force.
The dataset consists of 21,866 observations on 3,134 households.
It records household non-durable expenditures aggregated into 5 broad
commodity groups.
The price data are calculated from published prices aggregated to corre-
spond to the expenditure categories.
We check GARP individually for each member of the panel - no pooling.


--- Page 57 ---
Applying Afriat’s Theorem
Pass rate = 0.957


--- Page 58 ---
Applying Afriat’s Theorem
• Suppose we have some data {pt, qt}t=1,...,T which satisﬁes GARP.
• Since this individual has thus far been observed to behave in a way
which is perfectly consistent with utility maximisation, we can try to
recover their implied preferences.
• It is essential to understand that there may be more than one prefer-
ence relation which is consistent with the data (Afriat’s Theorem give
us one, there may be others).
• So the recoverability question focuses on identifying the set of prefer-
ences that are consistent with a given data set.


--- Page 59 ---
Applying Afriat’s Theorem
• Recoverability is based entirely on the restrictions upon behaviour im-
posed by GARP.
• The recoverability question aims at constructing inner and outer bounds
for the indiﬀerence curves passing through an arbitrary, not necessarily
observed, quantity bundle.
• The essential idea is to squeeze the indiﬀerence curve of interest be-
tween a set of bundles which are revealed preferred and a set which
are revealed worse.


--- Page 60 ---
Applying Afriat’s Theorem


--- Page 61 ---
Applying Afriat’s Theorem


--- Page 62 ---
Applying Afriat’s Theorem
• Suppose we have some data {pt, qt}t=1,...,T which satisﬁes GARP and
then present the consumer with a new budget {p0, x0}. What will the
consumer do?
• We will use the assumption that, since the individual has thus far been
observed to behave in a way which is perfectly consistent with utility
maximisation, she will continue to do so.
• Whatever she does, her new chosen bundle must therefore satisfy
GARP in combination with her previously-observed choices..
• Again there will typically be more than one bundle which satisﬁes this
restriction.


--- Page 63 ---
Applying Afriat’s Theorem
• The description of the forecast is that the new demand (q0) must lie
in the set deﬁned by
S (p0, x0) =
(
q0 :
q0 ≥0,
p′
0q0 = x0
{p0, pt; q0, qt}t=1,...,T satisﬁes GARP
)
• That is it must be
– non-negative (although corners are ﬁne)
– satisfy the budget constraint
– satisfy GARP when pooled with the observed choices.


--- Page 64 ---
Applying Afriat’s Theorem


--- Page 65 ---
Applying Afriat’s Theorem
• As the data becomes dense
– the RP test for consistency becomes more demanding
– the bounds on indiﬀerence curves become tighter
– the bounds on demand responses become tighter.
• If the data become perfectly dense (eﬀectively an inﬁnite dataset) we
have the indiﬀerence curve map and demand curves themselves.


--- Page 66 ---
Firms
• The most elementary way in which to describe a ﬁrm’s technology is
by means of its input requirement set V (y).
• This consists of all input vectors x that can produce at least the output
y
V (y) = {x : x can produce at least y}
• A key property of input requirement sets is that they must be nested:
Deﬁnition: Nestedness: If x is in V (y) and y ≥y′, then x is in
V
 y′ .


--- Page 67 ---
Firms
• The following are also typically assumed:
Deﬁnition: Monotonicity: If x is in V (y) and x′ ≥x, then x′ is
in V (y)
Deﬁnition: Convexity: If x ∈V (y) and x′ ∈V (y) , then
λx + (1 −λ) x′ ∈V (y)
where λ ∈[0, 1] .
Deﬁnition: Regularity: V (y) is a closed non-empty set for all y.


--- Page 68 ---
Firms
• Suppose that we observe a sequence of input vectors, input price vec-
tors and output production for a ﬁrm: {wt, xt, yt}t=1,...,T
• The most elementary behavioural hypothesis about the ﬁrm we can
entertain is that it is cost-minimising.
min
x w′tx such that x ∈V (yt)
– If the data were generated by the model what properties must the
data necessarily have?
– If the observed data with these properties in some data, is that
suﬃcient to know that the data could have been generated by the
model?


--- Page 69 ---
Firms
We are interested in the agreement between theory and data.
Deﬁnition: A family of input requirement sets V (y) rationalises
the data {wt, xt, yt}t=1,...,T if w′txt ≤w′tx for all x ∈V (yt).


--- Page 70 ---
Firms
Theorem†. The following statements are equivalent:
A. there exists a family of nested input requirement sets V (y)
which rationalises the data {wt, xt, yt}t=1,...,T.
B. the data satisﬁes the Weak Axiom of Cost-Minimisation: yt ≤
ys implies w′txt ≤w′txs
C. there exists a family of nested, convex, monotonic input require-
ment sets V (y) which rationalises the data {wt, xt, yt}t=1,...,T.
†Hanoch and Rothschild (1972), Diewert and Parkan (1979), Varian (1984).


--- Page 71 ---
Firms
• The empirical condition in statement (B) is known as the Weak Axiom
of Cost Minimisation (WACM).
• Compared to Afriat’s Theorem it is very straightforward to verify -
simply inspect the dataset and check the condition directly.
• The equivalence between (A) and (C) means that if the data can be
rationalised by any family of input requirement sets, then in fact it
can be rationalised by a ”nice” one and there is no harm is having
these extra properties.
• Another way to say the same thing is that, in a ﬁnite data setting,
these additional properties have no empirical content.


--- Page 72 ---
Firms
• We will take A to be true and show that it implies B (WACM).
• Let V (y) be a family of nested input requirement sets that rationalise
the data.
• If yt ≤ys then nestedness means that xs ∈V (yt).
• Since V (y) rationalises the data w′txt ≤w′sxs. Thus
yt ≤ys implies w′txt ≤w′txs


--- Page 73 ---
Firms
• Now we will show that if B holds then C holds.
• Again (like Afriat’s Theorem) the proof is constructive - we will build
a family of input requirement sets out of the raw materials: a dataset
{wt, xt, yt}t=1,...,T which satisﬁes WACM.
• Let V (y) be the positive convex monotonic hull of the {xt}t=1,...,T
such that yt ≥y
V (y) = com+{xt : yt ≥y}


--- Page 74 ---
Firms
• This construction is closed, convex and monotonic.
• We need to show that this construction rationalises the data. That is
that for any xt it is the case that w′txt ≤w′tx for all x ∈V (yt).
• The trick is to note that since the inequality of interest w′txt ≤w′tx
is linear we only need to worry about the vertices.


--- Page 75 ---
Firms
• The next point is to realise that the construction means that all of
the vertices are observed input bundles.
And we know that at the
observed bundles WACM (condition B) holds.
• Therefore w′txt ≤w′txs for all xs∈V (yt).
• So xt is itself a vertex and lies on a supporting hyperplane with slope
given by wt. Therefore for any xt it is the case that w′txt ≤w′tx.
• Lastly we know that C implies A since it is stronger. Thus we have
A ⇒B ⇒C ⇒A ⇒...


--- Page 76 ---
Firms
• Instead of considering a set-theoretic object we can use a production
function.
Deﬁnition: A production function f (x) rationalises the data {wt, xt, yt}t=1,...,T
if f(xt) = yt and f(x) ≥yt implies w′txt ≤w′tx for t = 1, ..., T.
• It is useful, in the context of production functions, to add the require-
ment of continuity.


--- Page 77 ---
Firms
Theorem. The following statements are equivalent:
A. there exists continuous production function which rationalises
the data {wt, xt, yt}t=1,...,T.
B1.
the data satisfy the Strong Axiom of Cost Minimisation:
ys ≤yt implies w′sxs ≤w′sxt
B2. there exist numbers {Ut, λt > 0}t=1,...,T such that
Us ≤Ut + λtw′t (ws −wt) ∀s, t ∈{1, ..., T}
C. there exists a continuous, monotonic and quasi-concave pro-
duction function f (x) which rationalises the data {wt, xt, yt}t=1,...,T.


--- Page 78 ---
Firms
• Competitive proﬁt maximisation is probably the canonial model in the
theory of the ﬁrm.
• We let yt denote an observed netput vector where yt = [y1t , ...yK
t , ] is
a signed vector of net outputs of K goods.
• So if the kth element is positive it’s an output, and if it’s negative it’s
an input.
• The prices (of inputs and outputs) are pt = [y1t , ...yK
t , ].


--- Page 79 ---
Firms
• Given the sign convention on netputs the ﬁrm’s proﬁt is just
p′tyt
• You can think of the netput vector being arranged as
yt =
"
yt
−xt
#
,
pt =
"
pt
wt
#
so
p′tyt = ptyt −w′txt


--- Page 80 ---
Firms
• The key theoretical object is the production set: the set of technically
feasible input-output combinations: Y.
• Generally we assume that Y is negative monotonic (free disposal).
This says
if y ∈Y and y′ ≤y then y′ ∈Y
• Once again we are interested in when there exists a production set
which rationalises a set of observations on a ﬁrm
Deﬁnition: A production set Y rationalises the data {pt, yt}t=1,...,T
if p′tyt ≥p′ty for all y ∈Y for all t = 1, ..., T.


--- Page 81 ---
Firms
Theorem‡. The following statements are equivalent:
A. there exists a production set which rationalises the data {pt, yt}t=1,...,T.
B. p′tyt ≥p′tys for all s, t = 1, ..., T
C. there exists a close convex negative monotonic production set
which rationalises the data {pt, yt}t=1,...,T.
‡Hanoch and Rothschild (1972), Diewert and Parkan (1979), Varian (1984).


--- Page 82 ---
Firms
• Statement (B) is known as the weak axiom of proﬁt maximisation
(WAPM).
• Firstly it is immediate that if (A) is true then (B) follows because the
production set must contain the observations {yt}t=1,...,T
• To show that if WAPM (B) holds, then that implies the existence of
a well-behaved production set (C) we construct one.


--- Page 83 ---
Firms
• Let Y = com−{yt}. This is the negative convex monotonic hull of
yt.
• To show that this rationalises the data consider an arbitrary point y
constructed as
y =
T
X
s=1
λs(ys + es)
where es ≤0 and PT
s=1 λs = 1.
• So y is any point which lies somewhere inside the constructed set. We
need to show that it gives a lower proﬁt than the chosen point.


--- Page 84 ---
Firms
• We know that condition (B) means that p′tyt ≥p′tys so
λsp′tyt ≥λsp′t(ys + es)
for all s, t
• Summing over s gives
T
X
s=1
λsp′tyt
≥


T
X
s=1
λsp′t(ys + es)


p′tyt
≥p′t


T
X
s=1
λs(ys + es)


p′tyt
≥p′ty


--- Page 85 ---
Firms
• When there is a single output there is a very straightforward version
of WAPM.
p′tyt ≥p′tys ⇒ptyt −w′txt ≥ptys −w′txs
• This can be rearrange to give
ptys
≤
ptyt + w′txs −w′txt
ys
≤
yt + 1
pt
w′t (xs −xt)
which is an Afriat Inequality composed of observables.


--- Page 86 ---
Firms
Theorem. The following statements are equivalent:
A. there exists a production function which rationalises the data
{wt, xt, yt, pt}t=1,...,T.
B. the data satisfy the Strong Axiom of Proﬁt Minimisation:
ys ≤yt + 1
pt
w′t (xs −xt)
∀s, t ∈{1, ..., T}
C. there exists a continuous, monotonic and concave production
which rationalises the data {wt, xt, yt, pt}t=1,...,T.


--- Page 87 ---
Firms
All of the sort of questions which we looked at in the context of individual
choices can be studies in the ﬁrm context.
For example we can forecast conditional factor demands given new input
prices and output levels using WACM, or predict netputs given a change
in the prices of inputs and or ﬁnal goods.
We can also test for imperfect competition and measure (in)eﬃciency.


--- Page 88 ---
Statistical Issues
• Revealed preference ”test” are deterministic: either the subject passes
in which case her behaviour can (heuristically) be regarded as being
that of a utility-maximiser, or she doesn’t in which case it cannot.
• This reﬂects the idea that the DGP is the deterministic model
max
q
u(q) subject to p′tq = yt
rather than a stochastic process.
• Important statistical considerations are present nonetheless.


--- Page 89 ---
Statistical Issues
1. we might only get to see sample of individuals from a larger population.
2. the data may very well be subject to measurement errors.
3. the individual may make optimisation errors which are stochastic in
nature.
• There is also an inferential question which arises even if we set aside
these other issues.


--- Page 90 ---
Afriat’s Theorem - A Bayesian View
• Under-determinism is a concept from the philosophy of science about
the relationship between theory and data.
”Most thinkers of any degree of sobriety allow, that an hypothesis
... is not to be received as probably true because it accounts for
all the known phenomena, since this is a condition sometimes ful-
ﬁlled tolerably well by two conﬂicting hypotheses...while there are
probably a thousand more which are equally possible, but which,
for want of anything analogous in our experience, our minds are
unﬁtted to conceive.”
J.S. Mill in A System of Logic ([1867] 1900, p328)


--- Page 91 ---
Afriat’s Theorem - A Bayesian View
• Suppose we observe a repeated observations on a single individual:
{pt, qt}t=1,...,T.
• Suppose they pass GARP. What are we to make of that?
• How justiﬁed might we be in thinking that this individual is, heuristi-
cally at any rate, really a utility-maximiser?


--- Page 92 ---
Afriat’s Theorem - A Bayesian View
• Clearly our assessment of this will depend on
1. the number of observations
2. the ability of the GARP test to detect non-rational behaviour.
• If we have few observations, or constraints which do not cross often,
then the evidence is probably weak and we should be unwilling to
conclude simply that since this person has passed GARP, she must be
a utility-maximiser.


--- Page 93 ---
Afriat’s Theorem - A Bayesian View
• We are interested in whether or not the individual is a utility-maximiser
(denoted U), given the data satisfy GARP (denoted G).
• Bayes’ Theorem gives
P(U|G)
=
P(G|U)P(U)
P(G)
=
=
P(G|U)P(U)
P(G|U)P(U) + P(G|¬U)[1 −P(U)]


--- Page 94 ---
Afriat’s Theorem - A Bayesian View
• Assuming no optimisation or measurement error then P(G|U) = 1
because a utility-maximiser will certainly pass GARP.
• So this gives us
P(U|G) =
P(U)
P(U) + P(G|¬U)[1 −P(U)]
where P(U) is the prior.
• Bayes’ Theorem tells us how to weigh the evidence.


--- Page 95 ---
Afriat’s Theorem - A Bayesian View
P(U|G) =
P(U)
P(U) + P(G|¬U)[1 −P(U)]
• If the GARP test is not very well able to detect non-rational behaviour
very well, then P(G|¬U) ≈1 and P(U|G) →P(U).
• This means that the evidence of the successful GARP test should not
impress us much and should do little to shift our prior beliefs.


--- Page 96 ---
Afriat’s Theorem - A Bayesian View
P(U|G) =
P(U)
P(U) + P(G|¬U)[1 −P(U)]
• If the GARP test is in fact very sensitive (for example, we have many
observations) and P(G|¬U) ≈0, then P(U|G) →1
• Consequently the GARP test gives us rational grounds to become very
conﬁdent that the individual is, in fact, a utility-maximiser.


--- Page 97 ---
Afriat’s Theorem - A Bayesian View
• The term P(G|¬U) is therefore centrally important to the way in which
we interpret a successful empirical GARP test.
• Its value depends on the alternative DGP (i.e. what ever ¬U is).
• The diﬃculty is that there are many alternatives to rational choice
models.


--- Page 98 ---
Afriat’s Theorem - A Bayesian View
• One important, non-rational alternative considered by Becker (1962)
was a probabilistic DGP: uniform random choice.
• Bronars (1987) applied this in an RP context by calculating the prob-
ability of observing a violation of GARP with this DGP operating on
the observed constraints.
• Bronars’ approach remains the most popular method but more recent
contributions (notably Andreoni, Gillen and Harbaugh (2013))), whilst
sticking with the idea of a probabilistic alternative DGP, consider more
data-driven alternatives to uniform random choice - they suggest draw-
ing from the empirical distribution of observed choices to allow for a
more realistic alternative.


--- Page 99 ---
Predictive Success
”... lack of variation in the price data limits the power of these methods”
Hal Varian (Econometrica, 1982, pp 966-7)


--- Page 100 ---
Predictive Success
P = the set of possible choices which satisfy the budget constraints.
S = the set of choices which also satisfy GARP.


--- Page 101 ---
Predictive Success
When we check RP conditions for an individual we look to see whether
their choices fall within the areas allowed by the restrictions.
The size of the target area provided by the restrictions is a sensible measure
of how demanding the restrictions are.
r ∈{0, 1} : the pass/fail indicator.
a ∈[0, 1] : the relative area of the predicted subset compared to the
outcome space.


--- Page 102 ---
Predictive Success
The simple hit/miss rate should not be the sole measure of the performance
of the theory (if it was, then nothing could do better than ”anything goes”).
• good theories combine good hit rates (high pass rates) with demanding
predictions (small areas);
• poor theories make imprecise predictions (large areas) which the data
fail to satisfy (low pass rates).
Suggestion: take account of both r and a.
This idea is due to Reinhard Selten (J. of Math Soc Sci, 1991) who devel-
oped it in the context of experimental game theory.


--- Page 103 ---
Predictive Success
Some suggested properties of a measure of predictive success m(r, a):
Monotonicity: m (1, 0) > m (0, 1) .
Equivalence of trivial theories: m (0, 0) = m (1, 1) .
Aggregability:. For every λ ∈[0, 1]
m (λr1 + (1 −λ) r2, λa1 + (1 −λ) a2) = λm (r1, a1)+(1 −λ) m (r2, a2) .


--- Page 104 ---
Predictive Success
Selten’s Theorem.
The function, m = r −a satisﬁes the ax-
ioms. If f
m (r, a) also satisﬁes these axioms, then there exist real
numbers {γ, δ > 0} such that f
m (r, a) = γ + δm.
Remarks on the Theorem
Selten (J. Math. Soc. Sci., 1991) provides an ordinal characterisation of
r−a where he replaces aggregability with a continuity axiom and an axiom
which says that the diﬀerence between theories should be a function of the
diﬀerences between r’s and a’s. He uses stronger monotonicity axioms.


--- Page 105 ---
Predictive Success
m →1 : demanding restrictions and data which satisfy them.
m →−1 : undemanding restrictions and yet the data fail to conform.
m →0 : the apparent accuracy of the data simply mirrors the size of the
target.


--- Page 106 ---
Predictive Success
An additional interpretation of m ≈0 is that the theory performs about
as well as a uniform random number generator
This interpretation provides a link between the measure discussed here
and the investigation of statistical power conducted by Bronars (Econo-
metrica,1987).
The alternative hypothesis is uniform random behaviour (as per Becker
J.Pol.E., 1962).
(1 −a) can be interpreted as P (¬G | uniform random behaviour)


--- Page 107 ---
Predictive Success
We return to the Spanish Continuous Family Expenditure Survey data (the
Encuesta Continua de Presupuestos Familiares - ECPF) we saw last week.
r
=
0.957
a
=
0.912
m
=
0.045


--- Page 108 ---
The distribution of ai


--- Page 109 ---
The distribution of mi


--- Page 110 ---
Predictive Success
• I’m not, of course, claiming that these particular results apply more
widely than the dataset/conditions studied here (more restrictive mod-
els, e.g. intertemporal models or HARP, seem to provide a great deal
of discipline on the data).
• But I am claiming that presenting results using these measures sheds
a great deal more light on the empirical performance of a theory than
does the uncorrected aggregate pass rate which is often reported in
the empirical literature.


--- Page 111 ---
Inference
• If the data involved are a random panel sample of households and
demands are measured without error, then inference about objects
like the proportion of households in the population which satisfy RP
restrictions is straightforward.
• A sample proportion can be viewed as the fraction of “successes” in
N independent Bernoulli trials with the same success probability p.
• The central limit theorem implies that for large N, the sample pro-
portion bp is normally distributed with mean p and standard deviation
q
p (1 −p) N so the statistic
z = (bp −p) /
q
p (1 −p) N ∼N (0, 1)


--- Page 112 ---
Inference
• Inference with repeated cross-sections from a heterogeneous popula-
tion is more diﬃcult.
• The issue here is that we do not see the same consumer twice, so we
cannot proceed on an consumer-by-consumer basis, checking the RP
conditions for each one as before.
• The object of interest remains the population proportion of consumers
who satisfy the RP conditions.
• However, this parameter depends on the joint distribution of choices
over diﬀerent budget sets and repeated cross-sectional data do not
reveal this: only its marginal distributions can be observed.


--- Page 113 ---
Inference
Suppose we have a ﬁxed population observed twice. In the ﬁrst observation
they are distributed along budget constraint a and in the second on b. We
observe the two distributions but not the joint.


--- Page 114 ---
Inference
Let the proportion of the population on each segment be given by a1+a2 =
1 and b1 + b2 = 1. The population parameter of interest is the proportion
of people who behave rationally (pass GARP).
Because we cannot track individuals (we don’t observe the joint distribu-
tion) we have to think about best- and worst-case scenarios.


--- Page 115 ---
Inference
To make things easier suppose that the population consists of 100 people.
And that there are 50 people in a1, 50 in a2, 40 in b1 and 60 in b2 How
many people fail GARP?


--- Page 116 ---
Inference
Worst case: There were 50 people in a1. Suppose all of them moved to
b2. There are 60 in total in b2 so the other ten would have had to have
come from a2. What about the 50 on a2? They can go anywhere they like
on b and none of them will violate GARP. The 50 from a1 are all violating
GARP so the proportion who are irrational is at most 0.5.


--- Page 117 ---
Inference
Best case: There are 60 people on b2. At most only 50 could have come
from a2 so at least 10 must have come from a1.
That means that 10
individual violated rationality and so the proportion who are irrational is at
least 0.1.


--- Page 118 ---
Inference
• Under these circumstances, the population parameter of interest is not
point identiﬁed. But we can bound the proportion of the population
which behave irrationally within the interval [0.1, 0.5] and hence the
rational proportion in [0.5, 0.9].
• Note that the actual distributions n the budget constraints don’t mat-
ter much – just the proportions in each “patch”.
• Hoderlein and Stoye (2013) show how to do inference on the sam-
ple analogue of this in the context of the Weak Axiom of Revealed
Preference (i.e. without transitivity).


--- Page 119 ---
Measurement Errors
• An important diﬀerence between structural econometrics and empirical
revealed preference lies in the absence of an error term in the latter.
• Certainly error terms rarely appear in revealed preference theory: there
is no mention of an error term in Afriat’s Theorem.
• But as soon as we attempt to take those revealed preference conditions
to data, errors can no longer necessarily be ignored.


--- Page 120 ---
Measurement Errors
• The most obvious situation arises when we consider measurement er-
rors, but identical issues arise when revealed preferences are applied
to statistical objects (like estimates of aggregate consumption as in
Browning (International Economic Review, 1989) or nonparametric
Engel curves as in Blundell, Browning and Crawford (Econometrica,
2003 and 2008)).
• In these cases the price-quantity data we observe is a function of a
random variable.
• This introduces a statistical element to empirical revealed preference
and forms an important link between revealed preference with struc-
tural econometrics.


--- Page 121 ---
Measurement Errors
• To illustrate the case for classical additive measurement error consider
the model
qt = q∗t + et
where q∗t denote the true values of demands and et is a vector of
classical measurement errors.
• Now the DGP is the deterministic economic model plus a stochastic
model.
• Suppose that we are interested in the null hypothesis that the true
data {pt, q∗t}t∈T satisfy GARP.


--- Page 122 ---
Measurement Errors
• If the RP conditions fail for the observed demands qt, it is possible
to generate a restricted estimator, bqt using the following Gaussian
quasi-likelihood ratio or minimum distance criterion function:
L =
min
{bqt}t∈T
T
X
t=1
(qt −bqt)′ Ω−1
t
(qt −bqt)
subject to the restriction that {pt, bqt}t∈T satisﬁes GARP and where
the weight matrix Ω−1
t
is the inverse of the covariance matrix of the
demands.
• The solution to this problem leads to demands bqt, which satisfy the
RP restrictions and which are unique almost everywhere.


--- Page 123 ---
Measurement Errors
• Evaluated at the restricted demands, the above distance function also
provides a test statistic for the RP conditions.
• It can be used for conducting conservative inference.
• This test falls within the general class of misspeciﬁcation tests inves-
tigated in Andrews and Guggenberger (2007, Section 7).


--- Page 124 ---
Optimisation Errors
• Instead of asking whether the outcome of an empirical RP test rep-
resents a statistically signiﬁcant departure from a DGP in to which
a stochastic element has been introduced, we can also ask whether
the results of the test represent an economically signiﬁcant departure
from rational choice.
• The key to this is to see that when a consumer violates RP conditions,
that consumer appears to waste money by buying a consumption bun-
dle when a cheaper bundle is available and also revealed preferred to
it.


--- Page 125 ---
Optimisation Errors
• The cost-eﬃciency measure suggested in Afriat (1967) is the smallest
amount of this wastage (as a fraction of the overall budget) consistent
with the given demand data.
• This index provides a simple way of measuring the size of a violation
of GARP and does so in units which are easy to understand and to
interpret economically.


--- Page 126 ---
Optimisation Errors
• The idea is to modify the revealed preference relation R, essentially
relaxing it to allow for some ineﬃcency by the consumer.
• Normally we deﬁne ”directly revealed preferred to” using p′tqt ≥
p′tqs ⇔qtR0qs and the transitive closure of R0 by R in the usual
way.
• Instead we say that qt is directly revealed preferred to qs at eﬃciency
level e using ep′tqt ≥p′tqs ⇔qtR0eqs and deﬁne the transitive closure
of this relation as Re in the usual way.
• Then we have GARPe is qtReqs implies not qtP 0e qs.


--- Page 127 ---
Optimisation Errors
• The number e can be thought of as how much less the potential
expenditure on a bundle has to be before we will consider it worse
than the observed choice.
• If e is 0.95, for example, we will only count bundles whose cost is less
than 95% of an observed choice as being revealed worse than that
choice.
• Said another way: if e is 0.95 and qs would cost only 2% less than qt
at pt prices we would not consider this a signiﬁcant enough diﬀerence
to conclude that qt was preferred by the consumer to.qs


--- Page 128 ---
Optimisation Errors
• We are allowing the consumer a ‘margin of error’ of (1 −e) .
• Afriat’s Critical Cost Eﬃciency Index, or the Afriat Eﬃciency Index for
short, is the largest value of e ∈[0, 1] such that there are no violations
of GARPe.


--- Page 129 ---
Optimisation Errors
• If e = 1 then there are no violations of GARP in the original data, but
for e < 1 there are violations.
• Traditionally, researchers begin their analysis of consumer behavior by
setting some critical level of e , say e∗; such that they would consider
any e > e∗a small or tolerable violation of GARP.
• Varian (1991), for instance, suggests a value of e = 0.95


--- Page 130 ---
Adding Structure
• In the basic model of rational demand that we discussed above, we
are considering any type of (well-behaved) utility function.
• But many models in economics depend critically on more particular
functional assumptions.
• For example: additive separability is essentially the deﬁning charac-
teristic of expected utility theory.
• If you want to investigate with particular structures or particular mod-
els using RP we need more than just Afriat’s Theorem.


--- Page 131 ---
Adding Structure: Separability
“Separability is about the structure we are to impose on our model: what to investigate
in detail, what can be sketched in with broad strokes without violence to the facts.”
W.M. Gorman (1987
• Separability is the most important restriction used in applicable theory.
• It refers to certain restrictions on functional representations of pref-
erences or technologies which add structure to the decision making
tasks undertaken by economic agents.
• These restrictions also allow the economic researcher to study the
behavior of these agents in a more eﬀective manner.)


--- Page 132 ---
Adding Structure: Separability
• Partition our data into two sets of goods and prices
nn
p1t, q1t
o
,
n
p2t, q2t
oo
t=1,...,T
• A utility function is separable in the group 1 goods, if
{q1, q2} ⪰
n
q1∗, q2o
⇔{q1, q2
#} ⪰
n
q1∗, q2
#
o
for all q1, q1∗, q2 and q2
#.
• That is preferences within group 1 are independent of the composition
of group 2.


--- Page 133 ---
Adding Structure: Separability
• The functional representation is that a utility function u is (weakly)
separable in the the group 1 goods if we can ﬁnd a ”subutility func-
tion” v

q1
and a ”macro function” w(v, q2) with w(v, q2) strictly
increasing in v such that:
u

q1, q2
= w(v(q1), q2)
• Separability confers two major simplifying beneﬁts:
1. the ability to ignore certain things,
2. dimension reduction


--- Page 134 ---
Adding Structure: Separability
• What revealed preference conditions would reﬂect this structure?
• Recall we need both necessary and suﬃcient conditions.
• Clearly the entire data set must satisfy GARP since it comes from
maximisation of u(q1, q2).


--- Page 135 ---
Adding Structure: Separability
• Weak separability is also necessary and suﬃcient for the second (lower)
stage of two-stage budgetting.
• The sub-dataset must satisfy GARP since each q1 must solve the
problem:
max
q1 v

q1
subject to p1′
t q1 = p1′
t q1t
• To see why suppose that q1∗satisﬁed the budget constraint and yielded
higher subutility. Then w(v

q1∗

, q2t) > w(v

q1t

, q2t) and p1tq1∗+
p2tq2t ≤p1tq1t + p2tq2t contradicting maximisation.


--- Page 136 ---
Adding Structure: Separability
• So the pooled data
n
p1t, p2t, q1t, q2t
o
t=1,...,T and the sub-group data
n
p1t, q1t
o
t=1,...,T must satisfy GARP but we also need to allow for the
aggregating/dimension-reducing aspect of separability.
• Concavity conditions for the macro and the sub-utility functions are
u

q1s, q2s

≤
u

q1t, q2t

+ ∇q1
t u

q1t, q2t
′ 
q1s −q1t

+∇q2
t u

q1t, q2t
′ 
q2s −q2t

v

q1s

≤
v

q1t

+ ∇v

q1t
′ 
q1s −q1t



--- Page 137 ---
Adding Structure: Separability
• Denote vt = v

q1t

.
• Then the concavity condition for w

v, q2
is is
w

vs, q2s

≤w

vt, q2t

+ ∂w
∂vt
(vs −vt) + ∇w

vt, q2t
′ 
q2s −q2t



--- Page 138 ---
Adding Structure: Separability
• Optimising behaviour (ﬁrst order conditions) gives us
∇q1
t u

q1t, q2t

≤
λtp1t
∇v

q1t
′
≤
µtp1t
∇q2
t u

q1t, q2t

≤
λtp2t
∇q2
t w

vt, q2t

≤
λtp2t
where λt is the marginal utility of income and µt is the marginal utility
of income allocated to the q1 group, that is the Legrange multiplier
on the problem
max
q1 v

q1
subject to p1′
t q1 = p1′
t q1t


--- Page 139 ---
Adding Structure: Separability
• Deﬁne
ρt = ∂w
∂vt
• Then the concavity conditions become
u

q1s, q2s

≤
u

q1t, q2t

+ λtp1′
t

q1s −q1t

+ λtp2′
t

q2s −q2t

v

q1s

≤
v

q1t

+ µtp1′
t

q1s −q1t

w

vs, q2s

≤
w

vt, q2t

+ ρt (vs −vt) + λtp2′
t

q2s −q2t



--- Page 140 ---
Adding Structure: Separability
• From the chain rule we have
∇q1
t u

q1t, q2t

= ∂w
∂vt
∇v

q1t

= ρtµtp1t
• We also had
∇q1
t u

q1t, q2t

≤λtp1t
• So
λt
µt
= ρt


--- Page 141 ---
Adding Structure: Separability
• The concavity/optimality conditions are therefore
u

q1s, q2s

≤
u

q1t, q2t

+ λtp1′
t

q1s −q1t

+ λtp2′
t

q2s −q2t

v

q1s

≤
v

q1t

+ µtp1′
t

q1s −q1t

w

vs, q2s

≤
w

vt, q2t

+ λt
µt
(vs −vt) + λtp2′
t

q2s −q2t

• The ﬁnal step just replaces the values of these real-valued functions
with real numbers.


--- Page 142 ---
Adding Structure: Separability
Theorem (Varian (1982), Afriat (1967)).
The following condi-
tions are equivalent:
(1) there exists a weakly separable, concave, monotonic, continu-
ous non-satiated utility function that rationalises the data;
(2) there exist numbers {Vt, Wt, λt > 0, µt > 0}t=1,...,T that sat-
isfy:
Vs
≤
Vt + µtp1′
t

q1s −q1t

Ws
≤
Wt + λt
µt
(Vs −Vt) + λtp2′
t

q2s −q2t

(3) the data
n
p1t, q1t
o
t=1,...,T and
n
1/µt, p2t, Vt, q2t
o
t=1,...,T sat-
isfy GARP for some choice of {1/µt, Vt}t=1,...,T that satisﬁes the
Afriat inequalities.


--- Page 143 ---
Adding Structure: Separability
• There is no explicit mention of a condition corresponding to an Afriat
condition for the entire dataset or the statement ”the entire dataset
satisﬁes GARP”.
• This is because that condition is implied by the other two [hint: add
the other inequalities up].
• There is also a new computational problem. This can be seen in two
ways which are equivalent.


--- Page 144 ---
Adding Structure: Separability
• The ﬁrst is that it is necessary to ﬁnd a set of Afriat numbers {1/µt, Vt}t=1,...,T
which represent a group quantity and price index such that
1. they satisfy the Afriat inequality Vs ≤Vt + µtp1′
t

q1s −q1t

2.
n
1/µt, p2t, Vt, q2t
o
t=1,...,T satisﬁes GARP
• The diﬃculty is that the numbers which satisfy (1), if they exist, are
not unique.


--- Page 145 ---
Adding Structure: Separability
• The second way to see the problem is to look at the Afriat inequality
Ws ≤Wt + λt
µt
(Vs −Vt) + λtp2′
t

q2s −q2t

• The issue is whether there exists a solution, or not.
• This is known as a ”certiﬁcation” problem.


--- Page 146 ---
Adding Structure: Separability
• For linear inequalities certiﬁcation is possible thanks (in theory) to
a result call Farkas Lemma and thanks (in practice) to the simplex
algorithm
• But these inequalities are non-linear in unknowns.
• There exists no certiﬁcation method which is guaranteed to converge
in a ﬁnite number of steps.


--- Page 147 ---
Adding Structure: Separability
• It is therefore possible in principle to determine whether or not data are
consistent with utility maximisation and a weakly separable preference
relation.
• But it is not always possible to show that data are not consistent with
that structure.
• Of course failure of just one of the necessary conditions would be
enough to rule it out.


--- Page 148 ---
More structure: additive separability
• A stronger assumption than weak separability, and one which is almost
as frequently made is additive separability.
• We say a utility function u(q1t, q2t) is additively separable if we can
write it as
u(q1t, q2t) = v(q1t) + w(q2t)
for some utility functions v(q1t) and w(q2t).


--- Page 149 ---
More structure: additive separability
• Since additive separability implies weak separability we know immedi-
ately that one condition is that there must exist some “Afriat numbers”
{Vt, Wt, λt > 0, µt > 0}t=1,...,T that satisfy:
Vs
≤
Vt + λtp1′
t

q1s −q1t

Ws
≤
Wt + µtp2′
t

q2s −q2t



--- Page 150 ---
More structure: additive separability
• The ﬁrst order conditions for overall utility maximisation imply that
∇q1
t u

q1t, q2t

=
∇w

q1t
′ ≤λtp1t
∇q2
t u

q1t, q2t

=
∇v

q2t
′ ≤λtp2t
• This means that the marginal utility of income is equalised across the
groups. Hence we can take λt = µt


--- Page 151 ---
More structure: additive separability
Theorem. The following two conditions are equivalent:
(1) there exist two concave, monotonic, continuous utility func-
tions whose sum rationalises the data;
(2) there exist numbers {Vt, Wt, λt > 0, µt > 0}t=1,...,T that sat-
isfy:
Vs
≤
Vt + λtp1′
t

q1s −q1t

Ws
≤
Wt + λtp2′
t

q2s −q2t

for all s, t ∈{1, ..., T}.


--- Page 152 ---
More structure: additive separability
• Once again there is no explicit condition which applies to the entire
dataset but such a condition is implied by the two conditions stated.
• There is no GARP-like condition for this model: you have to work
with the Afriat Inequalities.
• The Afriat conditions for additive separability are linear in unknowns
this is easy to implement - unlike the condition for weak separability.


--- Page 153 ---
Adding Structure: Returns to scale
• Constant returns to scale is another example of a functional restriction
(on a production function in this case) which is often useful to know
about.
• The basic result for proﬁt maximisation with a single-output produc-
tion function was SAPM:
ys ≤yt + 1
pt
w′t (xs −xt)
∀s, t ∈{1, ..., T}
• Obviously constant returns to scale is a special case of this so the data
will have to satisfy SAPM plus another condition.


--- Page 154 ---
Adding Structure: Returns to scale
• To brieﬂy recap the SAPM theorem we suppose that the production
function is concave: f (xs) ≤f (xt) + ∇f (xt)′ (xs −xt) where ys =
f (xs) and yt = f (xt).
• Optimising price-taking behaviour says that the ﬁrm should use inputs
until the marginal revenue product is equal to the price of each input:
pt∇f (xt) = wt
• So combining these gives SAPM:
ys ≤yt + 1
pt
w′t (xs −xt)


--- Page 155 ---
Adding Structure: Returns to scale
• A production function has constant returns to scale if it is homoge-
neous of degree one:
f (λx) = λf (x)
• Another very useful property of hod1 functions is that
f(x) = ∇f(x)′x
which is due (on the balance of probabilities if nothing else) to Euler.


--- Page 156 ---
Adding Structure: Returns to scale
• Using the optimality (pt∇f (xt) = wt) implies that
∇f (xt) = 1
pt
wt
• Therefore using the hod1 property we get
yt = f(xt) = ∇f(xt)′xt= 1
pt
w′txt


--- Page 157 ---
Adding Structure: Returns to scale
Theorem. The following statements are equivalent:
A. there exists a constant returns to scale production function
which rationalises the data {wt, xt, yt, pt}t=1,...,T.
B. the data satisfy the conditions
ys
≤
yt + 1
pt
w′t (xs −xt)
∀s, t ∈{1, ..., T}
yt
=
1
pt
w′txt
C. there exists a continuous, monotonic and concave constant re-
turns to scale production which rationalises the data {wt, xt, yt, pt}t=1,...,T.


--- Page 158 ---
Adding Structure: Returns to scale
• This way of presenting the result accentuates the fact that the CRS
structural assumption adds a condition to the empirical requirements:
the data has to satisfy SAPM plus another condition.
• You can (of course!) combine the two conditions
ys
≤
yt + 1
pt
w′txs −1
pt
w′txt
∀s, t ∈{1, ..., T}
ys
≤
1
pt
w′txs
∀s, t ∈{1, ..., T}


--- Page 159 ---
Adding Structure: Returns to scale
• Combining this with yt = 1
ptwt Varian (1984) takes ratios to get the
condition::
ys
yt
≤w′txs
w′txt
∀s, t ∈{1, ..., T}
• This is less clear economically, but computationally it is simpler.
• The RP test for homothetic utility functions is basically identical -
work it out as an exercise.


--- Page 160 ---
Adding Structure: Characteristics
• The linear characteristics model is due to Gorman (1956).
K market goods : q
J characteristics : z
J < K
max
q
v (z) subject to z = A′q and p′tq ≤xt
• The structure is akin to separability but with complete overlap across
groups rather than a partition.


--- Page 161 ---
Adding Structure: Characteristics
Deﬁnition: A utility function v (z) rationalises the data {pt, qt}t=1,...,T
for the technology A if v (zt) ≥v (z) for all z such that zt= A′qt,
z = A′q and p′tqt ≥p′tq.


--- Page 162 ---
Adding Structure: Characteristics
Theorem.The following statements are equivalent.
(P) there exists a utility function v (z) which is non-satiated, con-
tinuous and concave in characteristics which rationalises the data
{pt, qt}t=1,...,T for given A.
(A) there exist numbers {Vt, λt > 0}t=1,...,T and vectors {πt}t=1,..,T
such that
Vs ≤Vt + λtπ′t

A′qs −A′qt

(A1)
pt ≥Aπt

with equality if qkt > 0

(A2)


--- Page 163 ---
Adding Structure: Characteristics
• The model is
max
q
v (z) subject to z = A′q and p′tq ≤xt
• Maximising behaviour and linear structure
λtpt ≥A∇v (zt) = ∇u (qt)
πt = 1
λt
∇v (zt)
pt ≥Aπt
(A2)


--- Page 164 ---
Adding Structure: Characteristics
• Using the standard property of concave functions
v (zs) ≤v (zt) + ∇v (zt)′ (zs −zt)
• Given pt ≥Aπt and zt = A′qt
v (zs) ≤v (zt) + λtπ′t

A′qs −A′qt

Vs ≤Vt + λtπ′t

A′qs −A′qt

(A1)


--- Page 165 ---
Adding Structure: Characteristics
• We have T overestimates of the utility of an arbitrary bundle
V (z) ≤
n
Vs + λsπ′s (z −zs)
o
s=1,...,T
so take
V (z) = min
s
n
Vs + λsπ′s (z −zs)
o
s=1,...,T
as a utility function (it’s piecewise linear, non-satiated, and concave).


--- Page 166 ---
Adding Structure: Characteristics
• Suppose we have some z = A′q with p′tqt ≥p′tq.
V (z) = min
s
n
Vs + λsπ′s (z −zs)
o
≤Vt + λtπ′t (z −zt)
• Since p′t ≥π′tA′ with equality when qkt > 0
p′tqt ≥p′tq
⇒
π′
tzt ≥π′tz
• Since λt > 0 this means λtπ′t (z −zt) ≤0. So V (z) ≤Vt.


--- Page 167 ---
Adding structure - summary
• Separability and returns to scale are two examples in which the precise
structure of preferences or technology is of interest.
• The basic condition remains, but this is augmented with further in-
equality restrictions reﬂecting the extra assumption of interest.
• By switching on/oﬀthis additional condition we can conduct a ”spec-
iﬁcation” search for particular properties of the model.


--- Page 168 ---
Adding structure - summary
• The additional restriction also helps with prediction - the prediction
has to statisfy the basic conditions (e.g. GARP) but also the further
restrictions created by the additional structure.
• They therefore sharpen the bounds by reducing the size of the set of
theory-consistent observations.
• This emphasises the general point that : stronger theoretical restric-
tions give you stronger predictions.


--- Page 169 ---
Empirical Revealed Preference - summary
1. A “elementary” way of combining theory and data.
2. We have looked at
(a) The basic ideas/results
(b) Practical issues in implementation
(c) Adding structure to models


--- Page 170 ---
Empirical Revealed Preference - summary
The good things about RP include:
1. It is simple and clean from a theoretical point of view
2. It focuses analysis on the behaviour of the individual, i.e. the level at
which the theory applies, and not the behaviour of statistics.
3. It does not rely on or indeed require errors.
4. It introduces loss functions to empirical work which are more econom-
ically meaningful than the sum of squared residuals.


--- Page 171 ---
Empirical Revealed Preference - summary
The bad things about RP include (but are not limited to):
1. Bounds on objects of interest can be so wide that it is close to useless
for practical day-to-day empirical work.
2. It is hard to satisfactorily connect RP to standard econometric practice.
3. When a model fails you cannot just throw in an error
4. When a model doesn’t fail what should you make of that when “all
models are wrong”?


--- Page 172 ---
Thanks
Comments suggestions and queries to
ian.crawford@economics.ox.ac.uk
