--- Page 1 ---
CHAPTER 7
Stochastic Choice
We now study the empirical content of individual rational choice when choice
is stochastic. There are two possible interpretations of this exercise.
The ï¬rst is that we lack data on individual choices. There is instead
a population of agents, and we observe the distribution of choices in the
population. For example we may know how many people purchased Italian
wine in a wine store, and how many purchased French cheese in a cheese
store, but we do not know if those who bought the French product in one store
are the same people who bought Italian in the other. The theory to be tested
is that of rational agents with stable preferences. Thus we want to know when
an observed distribution of choices is consistent with a population of rational
agents with potentially different, but stable, preferences.
The second interpretation is that we observe an individual who literally
randomizes among different alternatives. We might observe this individual
agent over time, enough to infer a stochastic rule that he uses to select an
element at random when faced with a given set of available choices.
7.1
STOCHASTIC RATIONALITY
The model of stochastic choice can be described as follows. A system of choice
probabilities is a pair (X,P), where X is a ï¬nite set of alternatives and P is
a function with domain contained in 2X\{âˆ…} Ã— X, where P(A,x) = PA(x) is
a non-negative number for each nonempty A âŠ†X and x âˆˆA, and such that

xâˆˆA PA(x) = 1. In other words, PA(x) deï¬nes a probability distribution over
the set A.
In our ï¬rst interpretation of stochastic choice, there would be an underlying
large population of agents: PA(x) is the fraction of agents who choose x when
the set A of possible alternatives is available. In the second interpretation of
stochastic choice, an individual agentâ€™s choices really are random, and we
know that PA(x) is her probability of choosing x from A.
We assume here that all nonempty A âŠ†X are possible sets of alternatives
to choose from (they are budgets in the terminology of Chapters 2 and 3). We


--- Page 2 ---
96
Stochastic Choice
explain below (Remark 7.5) which results hold true when this assumption is
relaxed.
Given a ï¬nite set X, we consider the set  of all strict preferences on X. We
can identify each element of  with a one-to-one function Ï€ : X â†’{1,...,|X|};
such a function is a speciï¬c utility representation of the preference in question.
It is convenient in what follows to describe preferences using utility functions.
We use the following notational simpliï¬cation. Denote by Ï€(A) the set
{Ï€(x) : x âˆˆA} and write Ï€(x) â‰¥Ï€(A) to mean that Ï€(x) â‰¥Ï€(y) for all y âˆˆA.
Recall that the preferences in  are strict, so if x âˆˆA then Ï€(x) â‰¥Ï€(A) means
that x gives the highest utility in A for utility function Ï€.
A probability distribution on  is a function Î½ :  â†’R+ with 
Ï€âˆˆ Î½(Ï€)
= 1. Denote by () the set of all probability distributions on . When Î½ âˆˆ
() and E âŠ† then we write Î½(E) for 
Ï€âˆˆE Î½(Ï€).
A system of choice probabilities (X,P) is rationalizable if there is Î½ âˆˆ()
such that for all nonempty A âŠ†X and all x âˆˆA,
PA(x) =

Ï€âˆˆ
Î½(Ï€)1{Ï€(x)â‰¥Ï€(A)} = Î½ ({Ï€ âˆˆ : Ï€(x) â‰¥Ï€(A)}).
Rationalizability has different interpretations, depending on how we interpret
the stochastic choice and the system (X,P). If we interpret stochastic choice
as the choices of a population of agents, and PA(x) as the fraction of agents
that choose x from A, then rationalizability means that there is a population
distribution Î½ over the possible preferences that agents can have. Then the
fraction of agents choosing x is the fraction of agents for whom x is best in the
set A. If, on the other hand, PA(x) is the result of individual random choices,
then rationalizability means that the individualâ€™s preferences change. Given
a choice problem A, the individual agent draws a utility at random from ,
and chooses x with probability equal to the probability of drawing a utility for
which x is best in A. Given this interpretation, the model is often called a model
of random utility.
Before we go any further, it is worth setting down a very basic implication
of rationalizability:
Observation 7.1
If (X,P) is rationalizable, then for any x âˆˆX and nonempty
A,Aâ€² âŠ†X such that x âˆˆA âŠ†Aâ€², we have
PA(x) â‰¥PAâ€²(x).
The monotonicity property in Observation 7.1 is the stochastic counterpart
of Senâ€™s Î± (discussed in Chapter 2). The property is called regularity in the
literature on stochastic choice. Regularity is clearly too weak to characterize
rationalizable systems of choice probabilities. We turn instead to two stronger
properties: the axiom of revealed stochastic preference and the non-negativity
of the Blockâ€“Marschak polynomials.
A system of choice probabilities (X,P) satisï¬es the axiom of revealed
stochastic preference if, for all sequences (x1,A1),...(xn,An), with xi âˆˆAi for


--- Page 3 ---
7.1 Stochastic rationality
97
i = 1,...,n, we have that
n

i=1
PAi(xi) â‰¤max
Ï€âˆˆ
n

i=1
1{Ï€(xi)â‰¥Ï€(Ai)}.
Note that the sequence (x1,A1),...(xn,An) may repeat the same term (xi,Ai)
many times. As a consequence, testing the satisfaction of the axiom of revealed
stochastic preference is problematic because one must verify that an inï¬nite
number of sequences have the property above. This is never an issue when
using other revealed preference axioms, for example when testing for SARP
or GARP. It turns out, however, that there is an algorithm that inï¬nitely many
steps determines if the axiom is satisï¬ed. The algorithm amounts to checking
the existence of a solution of a system of linear inequalities.1
In addition to the axiom of revealed stochastic preference, a certain system
of polynomials turns out to characterize stochastic rationality. For all A âŠŠX
and x âˆˆAc = X \ A, deï¬ne the number Kx,A by
Kx,A =
|A|

i=0
(âˆ’1)|A|âˆ’i

{CâŠ†A:|C|=i}
PCc(x).
The collection of all Kx,A comprise the Blockâ€“Marschak polynomials for the
system of choice probabilities (X,P).
The meaning of the Blockâ€“Marschak polynomials is made clear by
Proposition 7.3 below. They are in principle difï¬cult to interpret. Note,
however, that a simple calculation gives:
Kx,{y} = PX\{y}(x) âˆ’PX(x),
for x Ì¸= y. So Observation 7.1 means that Kx,{y} â‰¥0 is necessary for
rationalizability. It turns out that, not only Kx,{y}, but all the Blockâ€“Marschak
polynomials must be non-negative for rationalizability; and conversely that
if they are all non-negative, then the system of choice probabilities is
rationalizable.
The following result collects two theorems, one due to McFadden and
Richter, and one due to Falmagne.
Theorem 7.2
Let (X,P) be a system of choice probabilities, where X is a
ï¬nite set. The following statements are equivalent:
I) (X,P) is rationalizable.
II) (X,P) satisï¬es the axiom of revealed stochastic preference.
III) The Blockâ€“Marschak polynomials for (X,P) are non-negative.
The role of the Block-Marschak polynomials may seem obscure. The
following result gives them a natural interpretation. We present this result
1 The existence of such solutions lies at the heart of many problems studied in this book: see the
discussion in Chapter 12.


--- Page 4 ---
98
Stochastic Choice
before the proof of Theorem 7.2 because it turns out to play a crucial rule
in the proof.
For C âŠ†X, and any x âˆˆCc, let
Mx,C = {Ï€ âˆˆ : Ï€(C) > Ï€(x) â‰¥Ï€(Cc)}
be the set of all utilities in  that rank any member of C above x, and x at the
top of Cc. Proposition 7.3 says that, if (X,P) is rationalizable, then Kx,A is the
probability that all the elements in A are ranked above x, and that x is at the top
of Ac; put differently, Kx,A is the probability that A is the upper contour set of x.
Proposition 7.3
The system of choice probabilities (X,P) is rationalized by
Î½ âˆˆ() iff Kx,A = Î½(Mx,A) for all (x,A).
Proof. The proof is an application of a combinatorial technique called
MÂ¨obius inversion; this speciï¬c type of MÂ¨obius inversion is called the
inclusionâ€“exclusion principle. The technique lets us invert variables which are
deï¬ned by cumulative sums of real-valued functions deï¬ned on a lattice. For
a set A and x âˆˆA, Î½ rationalizes the system of choice probabilities iff PA(x)
is the probability that the strict upper contour set of Ï€ at x is contained in Ac;
formally
PA(x) = Î½({Ï€ âˆˆ : {y : Ï€(y) > Ï€(x)} âŠ†Ac}).
Moreover, Î½(Mx,B) is by deï¬nition the probability that the strict upper contour
set of x is exactly B. Consequently, Î½ rationalizes the system of choice
probabilities iff PA(x) = 
BâŠ†Ac Î½(Mx,B), or inverting the role of A and Ac,
PAc(x) =

BâŠ†A
Î½(Mx,B).
(7.1)
Equation (7.1) shows that PAc is deï¬ned by a cumulative sum; namely, one
deï¬nes it by summing Î½(Mx,B) across all B âŠ†A. MÂ¨obius inversion tells us
that, conversely, if we know the value of the left-hand side of equation (7.1)
for every A, we can recover Î½(Mx,B) for all B. An explicit formula for this
inversion in this case is well known, and is called the inclusionâ€“exclusion
principle. The application of this principle in this environment gives exactly
Î½(Mx,A) = Kx,A. (See Proposition 2 of Rota (1964) and the Corollary (Principle
of Inclusionâ€“Exclusion) on p. 345.)
7.1.1
Proof of Theorem 7.2
The proof requires the following technical lemma, which we state here without
proof. Note that the ï¬rst part of the lemma is an alternative inductive deï¬nition
of the Blockâ€“Marschak polynomials.
Lemma 7.4
For all A âŠ†X:
I) Kx,A = PAc(x) âˆ’
CâŠŠA Kx,C if x âˆˆAc;
II) 
xâˆˆAc Kx,A = 
xâˆˆA Kx,A\{x}.


--- Page 5 ---
7.1 Stochastic rationality
99
We start by proving the equivalence of (I) and (III). The proof uses
Proposition 7.3 in important ways. First, note that the implication (I) â‡’
(III) is immediate from Proposition 7.3. We shall prove that (III) â‡’(I) by
constructing a rationalizing Î½ âˆˆ().
The structure of this proof is quite involved, so we divide it into steps.
Here is the basic structure. Our ultimate goal is to construct Î½ âˆˆ() which
rationalizes P. We do this by ï¬rst recursively constructing a set function Î½âˆ—on
a collection of â€œcylindersâ€ in . Let us consider Ï€âˆ—âˆˆ, and suppose x1 is the
Ï€âˆ—-maximal element, x2 is the second highest ranked element, and so forth. We
will ï¬rst deï¬ne Î½âˆ—, the probability of the set of all Ï€ for which x1 is maximal.
Using this number, we then ï¬nd the probability of the set of all Ï€ for which x1
is maximal and x2 comes second. Ultimately, this will allow us to construct the
probability of Ï€âˆ—.
We use the notation Î½âˆ—simply because the function is not deï¬ned on all
subsets of , but rather on the set of cylinders described in the previous
paragraph. But Î½âˆ—will then be deï¬ned on atoms (on each singleton {Ï€});
and will thus have a natural extension from the set of atoms to a probability
measure. This probability measure is Î½. Of course, Î½ and Î½âˆ—will coincide on all
cylinders. Along the way we will simply need to show that Î½(Ï€) â‰¥0 for every
Ï€, that the associated numbers add to one, and that we have rationalization.
Note that the cylinders referred to in the previous paragraph will be exactly the
type we need to consider in order to discuss rationalization.
Step 1: Deï¬ning the cylinders.
We use the term d-sequence for a sequence (x1,...,xk) such that all its terms
are (pairwise) distinct. For any d-sequence (x1,...,xk) let
S(x1,...,xk) = {Ï€ âˆˆ : Ï€(x1) > Ï€(x2) > Â·Â·Â· > Ï€(xk) > Ï€(X \ {x1,...,xk})}.
The set S(x1,...,xk) is the cylinder associated to (x1,...,xk). Let S be the collection
of all cylinders: the subsets of  of the form S(x1,...,xk), for some d-sequence
(x1,...,xk). We shall deï¬ne a function Î½âˆ—on S.
Step 2: Constructing Î½âˆ—, and verifying two important additivity properties.
We want to deï¬ne Î½âˆ—on S by induction. Let A = {x1,...,xk} for some
d-sequence (x1,...,xk) and let RA be the set of d-sequences of length k with
elements in A. The key properties required of Î½âˆ—will be the following.

(xâ€²
1,...,xâ€²
k)âˆˆRA
Î½âˆ—(S(xâ€²
1,...,xâ€²
k,x)) = Kx,{x1,...,xk}.
(7.2)
and

xâˆˆAc
Î½âˆ—(S(x1,...,xk,x)) = Î½âˆ—(S(x1,...,xk))
(7.3)
We proceed to deï¬ne Î½âˆ—. The guiding principle in the deï¬nition of Î½âˆ—is the
interpretation of Kx,A obtained from Proposition 7.3. We seek to construct Î½ so
that Kx,A is the probability that A is the strict upper contour set of x. The same
guiding principle suggests properties (7.2) and (7.3).


--- Page 6 ---
100
Stochastic Choice
First, for every x âˆˆX, let Î½âˆ—(S(x)) = Kx,âˆ…. Second, for every x,y âˆˆX,
with x Ì¸= y, let Î½âˆ—(S(y,x)) = Kx,{y}. Suppose now that Î½âˆ—(S(y1,...,yl)) has
been deï¬ned for all d-sequences (y1,...,yl) with l â‰¤k. Fix a d-sequence
(x1,...,xk,xk+1). Let A = {x1,...,xk} and RA be the set of all d-sequences in
A. If 
(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) = 0, let Î½âˆ—(S(x1,...,xk,xk+1)) = 0. Otherwise, let
Î½âˆ—(S(x1,...,xk,xk+1)) = Î½âˆ—(S(x1,...,xk))Kxk+1,{x1,...,xk}

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)).
(7.4)
This deï¬nes Î½âˆ—on S by induction.
We shall prove that (7.2) and (7.3) are satisï¬ed. The proof is by induction on
the length of the d-sequence deï¬ning A. It is easy to see by a direct calculation
that (7.2) and (7.3) are satisï¬ed for all sequences of length 0 and 1, where a
sequence of length 0 is associated to A = âˆ…, and we deï¬ne Î½âˆ—(Sâˆ…) = 1.
Suppose that (7.2) holds for all d-sequences of length l â‰¤k âˆ’1. Let A =
{x1,...,xk} for some d-sequence (x1,...,xk), of length k. Then we know that

(xâ€²
1,...,xâ€²
k)âˆˆRA
Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) =

xâˆˆA
â›
âœâ

(xâ€²
1,...,xâ€²
kâˆ’1)âˆˆRA\{x}
Î½âˆ—(S(xâ€²
1,...,xâ€²
kâˆ’1,x))
â
âŸâ 
(7.5)
=

xâˆˆA
Kx,A\{x}
(7.6)
=

xâˆˆAc
Kx,A.
(7.7)
Here, (7.5) follows from the inductive hypothesis and (7.3) (by adding over the
right-hand-side of (7.3)). Furthermore, (7.6) is a consequence of the inductive
hypothesis and (7.2); and (7.7) follows from the second part of Lemma 7.4.
Step 2a: Verifying the two additivity properties ((7.2) and (7.3)) in the case

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) = 0.
There
are
two
cases
to
consider.
Suppose
ï¬rst
that

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) = 0. Then (7.7) implies that for any x âˆˆAc,
we have Kx,A = 0, as all Block-Marschak polynomials are non-negative (the
hypothesis). Therefore, as by deï¬nition of Î½âˆ—, 
(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k,x)) = 0,
we have 
(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k,x)) = Kx,A. This veriï¬es (7.2).
Further, Equation (7.3) is clearly satisï¬ed when 
(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k))
= 0, because, by deï¬nition, Î½âˆ—(S(x1,...,xk,x)) = 0.
Step 2b: Verifying the two additivity properties ((7.2) and (7.3)) in the case

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) > 0.
The second case to consider is when 
(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) > 0.
By deï¬nition of Î½âˆ—(S(x1,...,xk,x)), Equation (7.2) is always satisï¬ed when


--- Page 7 ---
7.1 Stochastic rationality
101

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k)) > 0. To see that (7.3) also holds, note that:

xâˆˆAc
Î½âˆ—(S(x1,...,xk,x)) = Î½âˆ—(S(x1,...,xk))
xâˆˆAc Kx,{x1,...,xk}

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k))
= Î½âˆ—(S(x1,...,xk))
xâˆˆA Kx,A\{x}

(xâ€²
1,...,xâ€²
k)âˆˆRA Î½âˆ—(S(xâ€²
1,...,xâ€²
k))
= Î½âˆ—(S(x1,...,xk)).
The second equality above follows from the second property in Lemma 7.4;
the third equality follows from Equation (7.6).
This ï¬nishes the proof that Î½âˆ—satisï¬es (7.2) and (7.3).
Step 3: Deï¬ning Î½ from the Î½âˆ—(Ï€), and verifying that they coincide on
cylinders.
Now, Î½ can be deï¬ned on  in the following way. If we ï¬x Ï€ âˆˆ
then S(x1,...,x|X|) = {Ï€}, where (x1,...,x|X|) is the sequence deï¬ned by Ï€(xl) >
Ï€(xl+1), for l = 1...,|X| âˆ’1. We write Î½(Ï€) for Î½âˆ—(S(x1,...,x|X|)) and let Î½ be
the obvious extension of this measure to all subsets of , namely Î½(E) =

Ï€âˆˆE Î½(Ï€). Equation (7.3) establishes that Î½âˆ—= Î½ on S. Moreover, Î½ â‰¥0,
and it is easily veriï¬ed that 
Ï€âˆˆ Î½({Ï€}) = 1, so it is a probability measure.2
Finally, the fact that Î½(Mx,A) = Kx,A is the content of Equation (7.2). By
Proposition 7.3, the proof is complete.
We proceed to prove that (I) is equivalent to (II). The proof is a direct
application of a version of Farkasâ€™ Lemma.
Note ï¬rst that (I) implies (II) because, for any sequence (x1,A1),...,(xn,An),
rationalizability implies n
i=1 PAi(xi) â‰¤maxÎ½âˆˆ()
n
i=1 Î½({Ï€ : Ï€(xi) â‰¥
Ï€(Ai)}). The latter expression exhibits a linear function being maximized over
a compact and convex set. Hence, there is a maximizer at an extreme point;
in this case, there is a maximizer at some Î´Ï€âˆ—âˆˆ(), where Î´Ï€âˆ—is the point
mass on {Ï€âˆ—}. Then Î´Ï€âˆ—({Ï€ : Ï€(xi) â‰¥Ï€(Ai)}) = 1{Ï€âˆ—(xi)â‰¥Ï€âˆ—(Ai)}, concluding this
direction.
Conversely, let W be a matrix that has one column for every Ï€ âˆˆ and
one row for every pair (x,A) with x âˆˆA. In the entry corresponding to row
(x,A) and column Ï€ we have a zero if there is y âˆˆA with Ï€(y) > Ï€(x) and a
one otherwise; so the entry is 1{Ï€(x)â‰¥Ï€(A)}. The matrix W can be represented as
follows:
â¡
â¢â¢â£
Ï€1
Â·Â·Â·
Ï€|X|!
...
...
Â·Â·Â·
...
(x,A)
1{Ï€1(x)â‰¥Ï€1(y)âˆ€yâˆˆA}
Â·Â·Â·
1{Ï€|X|(x)â‰¥Ï€|X|!(y)âˆ€yâˆˆA}
...
...
Â·Â·Â·
...
â¤
â¥â¥â¦
2 Indeed, using (7.6) we obtain that (âˆ’1)Î½(Ï€) = |X|âˆ’1
i=0
(âˆ’1)|X|âˆ’i|X|
i

= âˆ’1, by the
Binomial Theorem.


--- Page 8 ---
102
Stochastic Choice
Let p be the vector with as many entries as there are pairs (x,A), whose entries
of p are arranged in the same order as the rows of W, so that PA(x) is the entry
in position (x,A) of the vector p.
Then we can represent a probability distribution Î½ âˆˆ() as a vector with
one entry for every Ï€ âˆˆ. The existence of a rationalizing Î½ is the same as the
existence of a solution Î½ to the system
p = W Â· Î½,
such that Î½ â‰¥0 and Î½ Â· (1,...,1) = 1.
By Farkasâ€™ Lemma (Lemma 1.14), there is no solution to the system iff there
is a vector Î· and a scalar Î¸ such that
Î· Â· W + Î¸(1,...,1) â‰¤0
(7.8)
Î· Â· p + Î¸ > 0.
(7.9)
We proceed to show ï¬rst that a violation of the axiom of revealed stochastic
preference follows from the existence of a solution to (7.8)â€“(7.9) in which
the entries of Î· are non-negative integers (in fact the two statements are
equivalent).
Let Î· and Î¸ be a solution to (7.8)â€“(7.9) in which the entries of Î· are
non-negative integers. Deï¬ne a sequence (x1,A1),...(xn,An) by including (in
any order) Î·(x,A) times the term (x,A); the sequence must have at least one term
since a solution Î· to (7.8)â€“(7.9) cannot be the null vector. Then n
i=1 PAi(xi) =

(x,A) Î·(x,A)PA(x) and, for any Ï€, n
i=1 1{Ï€(xi)â‰¥Ï€(Ai)} = 
(x,A) Î·(x,A)1{Ï€(x)â‰¥Ï€(A)}.
Since Î· and Î¸ solve (7.8)â€“(7.9) we obtain that
n

i=1
PAi(xi) + Î¸ > 0 â‰¥
n

i=1
1{Ï€(xi)â‰¥Ï€(Ai)} + Î¸,
for all Ï€. Thus the sequence (x1,A1),...(xn,An) presents a violation of the
axiom.
Finally, we prove that if there is a solution to (7.8)â€“(7.9) then we can take
the entries of Î· to be non-negative integers. We show how to reduce the system
to a collection of strict inequalities in Î·, by substituting out Î¸. Then we can
take Î· to have rational entries and satisfy the system. After multiplying Î· by a
large enough positive integer, we can assume that its entries are integers. We
shall prove that they can be assumed to be non-negative.
To see how to substitute out Î¸, note that Equations (7.8) and (7.9) imply
that, for every Ï€,

(x,A)
Î·(x,A)1{Ï€(x)â‰¥Ï€(A)} + Î¸ â‰¤0 <

(x,A)
Î·(x,A)PA(x) + Î¸.
Hence for all Ï€,

(x,A)
Î·(x,A)1{Ï€(x)â‰¥Ï€(A)} <

(x,A)
Î·(x,A)PA(x).
(7.10)


--- Page 9 ---
7.2 Luceâ€™s model
103
(In fact (7.10) being true for every Ï€ is necessary and sufï¬cient for the
existence of Î¸ that satisï¬es Equations (7.8) and
(7.9), by setting Î¸ =
âˆ’maxÏ€âˆˆ

(x,A) Î·(x,A)1{Ï€(x)â‰¥Ï€(A)} for a solution to (7.10).)
So it is without loss of generality to assume that Î· is integer-valued.
We show that we can take Î· â‰¥0 in (7.10) by showing that whenever Î·(Ë†x,Ë†A) < 0
then Equation (7.10) holds for some Î·â€² with Î·â€²
(Ë†x,Ë†A) = 0 and Î· â‰¤Î·â€².
Suppose
Î·(Ë†x,Ë†A) < 0.
Note
that
for
any
Ï€ âˆˆ,
1{Ï€(Ë†x)â‰¥Ï€(Ë†A)} =
1 âˆ’
zâˆˆË†A\{Ë†x} 1{Ï€(z)â‰¥Ï€(Ë†A)} and PË†A(Ë†x) = 1 âˆ’
zâˆˆË†A\{Ë†x} PË†A(z). Consequently
we get
Î·(Ë†x,Ë†A)1{Ï€(Ë†x)â‰¥Ï€(Ë†A)} = Î·(Ë†x,Ë†A) + (âˆ’Î·(Ë†x,Ë†A))

zâˆˆË†A\{Ë†x}
1{Ï€(z)â‰¥Ï€(Ë†A)}
and
Î·(Ë†x,Ë†A)PË†A(Ë†x) = Î·(Ë†x,Ë†A) + (âˆ’Î·(Ë†x,Ë†A))

zâˆˆË†A\{Ë†x}
PË†A(z).
So now ï¬nd Î·â€² as follows. Add âˆ’Î·(Ë†x,Ë†A) to both sides of Equation (7.10) for all Ï€
and make the preceding substitutions. Hence, Î·â€² coincides with Î· everywhere
except that Î·(Ë†x,Ë†A) = 0 and for all z âˆˆË†A\{Ë†x} Î·â€²
(z,Ë†A) = Î·(z,Ë†A) âˆ’Î·(Ë†x,Ë†A), we obtain that
Î·â€² satisï¬es (7.10) for all Ï€ âˆˆ while Î· â‰¤Î·â€² and Î·â€²
(Ë†x,Ë†A) = 0.
Remark 7.5
The proof that (I) is equivalent to (II) in the preceding does
not rely on the ability to observe the entire system of choice probabilities. In
particular, the axiom of revealed stochastic preference is also necessary and
sufï¬cient for rationalization by Î½ âˆˆ() for environments as in Chapter 2,
whereby we only observe PA for A in some set of budgets  âŠ†2X\{âˆ…}.
7.2
LUCEâ€™S MODEL
We now analyze the special class of systems (X,P) introduced by Duncan
Luce. The model is heavily used in applied work, and it lies at the foundation of
statistical and econometric studies of discrete choice. We proceed to describe
the model, and study its relation to stochastic rationality.
A system of choice probabilities (X,P) satisï¬es Luceâ€™s independence of
irrelevant alternatives (LIIA) if for any A, and any x,y âˆˆA, PA(x)P{x,y}(y) =
P{x,y}(x)PA(y). The LIIA axiom is easiest to interpret when the probabilities
involved are non-zero, so we can divide and obtain
PA(x)
PA(y) = P{x,y}(x)
P{x,y}(y).
So the LIIA axiom says that the likelihood of choosing x relative to y is
independent of what other alternatives may be available in A.
An example illustrates that LIIA may be unreasonable. Consider an agent
facing the set of alternatives {car,bus1}, who chooses each with probability
1/2. If the agent faces instead the set {car,bus1,bus2}, where the two buses only


--- Page 10 ---
104
Stochastic Choice
differ in their color (they go to the same place at the same speed), then we might
expect him to choose the car or either of the two buses with probability 1/2.
LIIA, however, implies that he must choose each alternative with probability
1/3.
LIIA has a clear implication. For notational simplicity, write qx,y for P{x,y}(x).
Suppose that PA(x) > 0 for all x âˆˆA, and for all nonempty A. Fix an element
z âˆˆX. Then we can deï¬ne u(x) = qx,z/qz,x.
Note that LIIA implies that 1 = 
xâˆˆA PA(x) = PA(y)
xâˆˆA
qx,y
qy,x . Note also
that
qx,y
qy,x
= Px,y,z(x)
Px,y,z(y) = Px,y,z(z)qx,z/qz,x
Px,y,z(z)qy,z/qz,y
= u(x)
u(y).
Then,
PA(y) =
u(y)

xâˆˆA u(x).
(7.11)
We say that (X,P) conforms to the Luce model if there is a function u : X â†’
R+ such that (7.11) holds for all A and y. We can make the interpretation of u
a bit more precise. Say that x âª°âˆ—y if qx,y â‰¥1/2. If (X,P) conforms to Luceâ€™s
model then x âª°âˆ—y iff u(x) â‰¥u(y). So âª°âˆ—is a preference relation represented
by u.
The numbers u(x) can be thought of as utility intensities. In previous
chapters, the utility functions have played a purely ordinal role in choice. But in
the Luce model, an alternative that has a higher utility than another alternative
has a higher probability of being chosen. So the utility function u conveys a
meaning above and beyond how it orders the different objects of choice.
The â€œrevealed preferenceâ€ problem of testing whether (X,P) conforms to
Luceâ€™s model is very simple to solve: set u(x) = PX(x) and verify whether the
resulting u satisï¬es the deï¬nition. Instead of testing Luceâ€™s model, we focus
on the relation between the model and stochastic rationality, as described in
Section 7.1.
Theorem 7.6
If (X,P) conforms to Luceâ€™s model, then it is rationalizable.
Before proving Theorem 7.6 we show that Luceâ€™s model does not exhaust
all the rationalizable systems of choice probabilities. Consider the following
example.
Let X = {a,b,c}. Suppose that the utility of a is given by a random variable
Ëœa; and that there are random variables Ëœb and Ëœc that deï¬ne the utilities of b and
c. Suppose that the three random variables, Ëœa, Ëœb and Ëœc, are independent and
distributed on {1,...,6} according to the following table:
1
2
3
4
5
6
Ëœa
0
0
1/2
1/2
0
0
Ëœb
0
0.6
0
0
0
0.4
Ëœc
0.4
0
0
0
0.6
0.


--- Page 11 ---
7.2 Luceâ€™s model
105
The distributions of Ëœa, Ëœb, and Ëœc describe a probability distribution on , as
any speciï¬cation of random utilities is equivalent to a probability distribution
on .
Then qa,b = 0.6, qb,c = 0.4 + 0.6 Ã— 0.4 = 0.64, and qc,a = 0.6. So a â‰»âˆ—b,
b â‰»âˆ—c and c â‰»âˆ—a. Then âª°âˆ—cannot have any utility representation, let alone
one that allows (X,P) to conform with the Luce model.
7.2.1
Proof of Theorem 7.6
Suppose that (X,P) conforms to Luceâ€™s model with a corresponding function
u; by a normalization we can suppose that 
xâˆˆX u(x) = 1. We use the same
notation as in the proof of Theorem 7.2.
For any Ï€ âˆˆ and j, let xÏ€
j denote the alternative in X with the jth highest
value in Ï€. Thus, Ï€(xÏ€
1 ) = |X|, Ï€(xÏ€
2 ) = |X| âˆ’1, . . . Ï€(xÏ€
|X|) = 1.
The proof proceeds by constructing a probability space in which one can
calculate the probability that a sequence of random draws will correspond to a
preference Ï€. Consider a probability space deï¬ned as follows. Draw inï¬nite
sequences in X at random by drawing independently (with replacement)
elements from X such that each z is drawn with probability u(z). Let Î¼ be
the associated probability measure on X. It should be clear that for any A âŠ†X
and x âˆˆA, the probability that x is drawn before any other element in A is equal
to u(x)/
yâˆˆA u(y).3
For any sequence j1 < ... < j|X| with j1 = 1, let Dj1,...,j|X|(Ï€) denote
the event that xÏ€
k is drawn for the ï¬rst time at draw number jk. Then

j1<...<j|X| Dj1,...,j|X|(Ï€) is the event C(Ï€) that the draws will conform to Ï€: so
C(Ï€) = 
j1<...<j|X| Dj1,...,j|X|(Ï€) is the event where xÏ€
1 is drawn ï¬rst; followed
by xÏ€
2 (possibly after several repeated draws of xÏ€
1 ); followed by xÏ€
3 (possibly
after several repeated draws of xÏ€
1 and xÏ€
2 ), and so on. The sets Dj1,...,j|X|(Ï€) are
disjoint, so the probability that the draws will conform to Ï€ is
Î¼(C(Ï€)) =

j1<...<j|X|
Î¼(Dj1,...,j|X|(Ï€))
=

j1<...<j|X|
u(xÏ€
1 )j2âˆ’j1u(xÏ€
2 )

u(xÏ€
1 ) + u(xÏ€
2 )
j3âˆ’j2âˆ’1
Â· u(xÏ€
3 )

u(xÏ€
1 ) + u(xÏ€
2 ) + u(xÏ€
3 )
j4âˆ’j3âˆ’1 Â·Â·Â·u(xÏ€
|X|).
The events C(Ï€) form a partition. Deï¬ne Î½({Ï€}) to be the probability of
C(Ï€). We can explicitly calculate Î½ as follows. Let hk = jk+1 âˆ’jk âˆ’1 and
3 To see this: Let E be the event that x is drawn before any other element in A. Then E occurs
if either x is obtained in the ï¬rst draw, which has probability u(x), or else an element of Ac is
obtained in the ï¬rst draw (which has probability 1 âˆ’
yâˆˆA u(y)) and then E occurs. Then the
probability q of E obeys the equation q = u(x) + (1 âˆ’
yâˆˆA u(y))q.


--- Page 12 ---
106
Stochastic Choice
vÏ€
k = k
l=1 u(xÏ€
l ). Then,
Î½(Ï€) = Î¼(C(Ï€)) =
"
zâˆˆX
u(z)

h1â‰¥0,...h|X|âˆ’1â‰¥0
(vÏ€
1 )h1(vÏ€
2 )h2 Â·Â·Â·(vÏ€
|X|âˆ’1)h|X|âˆ’1
=
"
zâˆˆX
u(z)
|X|âˆ’1
"
k=1
1
1 âˆ’vÏ€
k
=
"
zâˆˆX
u(z)
|X|âˆ’1
"
k=1
1
|X|
l=k+1 u(xÏ€
l )
=
|X|âˆ’1
"
k=1
u(xÏ€
k )
|X|
l=k u(xÏ€
l )
;
where the next-to-last equality follows by distributing the product 
zâˆˆX u(z)
appropriately, and using that |X|
l=1 u(xÏ€
l ) = 1.
Finally, we have already observed that the probability of drawing x before
any other alternative in A is equal to u(x)/
yâˆˆA u(y) = PA(x). Note that this
probability also equals 
{Ï€:Ï€(x)â‰¥Ï€(A)} Î¼(C(Ï€)) = Î½({Ï€ : Ï€(x) â‰¥Ï€(A)}). Hence
Î½ rationalizes (X,P).
7.2.2
Luceâ€™s model and the logit model
Luceâ€™s model can be interpreted as a random utility model in which the
â€œaverageâ€ utility of x is some known quantity v(x), but where the actual utility
is v(x) + Îµ(x), where Îµ(x) is unknown and random.
The following calculation shows a method of determining v, and motivates
why we can think of Luceâ€™s model as the â€œlogit model.â€ Suppose that (X,P)
conforms to Luceâ€™s model, with utility index u. Note that
qx,y =
u(x)
u(x) + u(y) =
u(x)/u(y)
u(x)/u(y) + 1 =
ev(x)âˆ’v(y)
1 + ev(x)âˆ’v(y) ,
where v(x) = log(u(x)). Then the probability of choosing x over y is qx,y =
Ï•(v(x) âˆ’v(y)). The function Ï• is the logistic distribution function. Then
PA(x) =
ev(x)

yâˆˆA ev(y) .
(7.12)
In particular, Equation (7.12) alternatively derives from a particular random
utility model speciï¬cation, which is called the logit model. Suppose that PA(x)
equals the probability that v(y) + Îµ(y) â‰¤v(x) + Îµ(x) for all y âˆˆA, y Ì¸= x.
That is, the probability that Îµ(y) âˆ’Îµ(x) â‰¤v(x) âˆ’v(y) for all y âˆˆA, y Ì¸= x.
Let v : X â†’R be an arbitrary function, and let G be a cdf on R, from
which the terms Îµ(x) are drawn independently across x. In particular, if G is
a Gumbel distribution, then qx,y, the probability of choosing x over y, is equal
to the probability that a logistic random variable is below v(x) âˆ’v(y). The


--- Page 13 ---
7.2 Luceâ€™s model
107
choice of a Gumbel distribution is suggested by the function Ï• being logistic.
The following proposition establishes that this speciï¬cation implies the choice
probabilities in Equation (7.12). It can therefore be viewed as a counterpart to
Theorem 7.6.
Proposition 7.7
Let v : X â†’R, and suppose that G(Î±) = exp(âˆ’exp(âˆ’Î±)).
Let Îµ be a random vector drawn from RX according to |X| independent draws
of G. Let (X,P) be deï¬ned by
PA(x) = Pr(v(y) + Îµ(y) â‰¤v(x) + Îµ(x) for all y âˆˆA4).
Then (X,P) conforms to Luceâ€™s model with index u(x) = ev(x).
Proof. Fix A and x âˆˆA. Let u(y) = ev(y) and Î´(y) = eâˆ’Îµ(y). Note that the form
of G implies that the distribution function of Î´(y) is Pr(Î´(y) â‰¤Î±) = 1 âˆ’eâˆ’Î±
(the exponential distribution). Then, by the deï¬nition of (X,P),
PA(x) = Pr(u(x)/Î´(x) â‰¥u(y)/Î´(y) for all y âˆˆA)
=
, âˆ
0
Pr(Î´(y) â‰¥Â¯Î´u(y)/u(x) for all y âˆˆA)eâˆ’Â¯Î´dÂ¯Î´
=
, âˆ
0
â›
â"
yâˆˆA\{x}
eâˆ’Â¯Î´u(y)/u(x)
â
â eâˆ’Â¯Î´dÂ¯Î´
=
, âˆ
0
exp
â§
â¨
â©âˆ’Â¯Î´
â›
â1 +

yâˆˆA\{x}
u(y)
u(x)
â
â 
â«
â¬
â­dÂ¯Î´
=
u(x)

yâˆˆA u(y).
Proposition 7.7 relies on a particular distribution for the random utility term Îµ.
One may ask if there are other distributions for Îµ that lead to the Luce model.
We provide a partial answer in the next result (which is due to McFadden),
where we show that if we insist on G being translation complete and the utility
index being onto, then the distribution giving rise to the Luce model is unique.
Consider now a system of choice probabilities (X,P), where we depart from
the assumptions in this chapter by allowing that X may be inï¬nite. Suppose that
PA(x) is only deï¬ned for ï¬nite sets A. As before, x â†’PA(x) is a probability
distribution on A.
Assume that PA(x) is obtained from a random utility model, as above.
Speciï¬cally, suppose that there is a utility v deï¬ned on A, and random
variables Îµ(x) such that x is chosen from A if v(y) + Îµ(y) < v(x) + Îµ(x) for all
y âˆˆA. Suppose that the random variables Îµ(x) are independent with identical
distribution function G on R. Say that (X,P) is generated from v : X â†’R
and G.
4 Here, Pr is probability calculated according to independent draws from G.


--- Page 14 ---
108
Stochastic Choice
The distribution function G is translation complete if, for any function f
such that
0 = lim
xâ†’âˆ’âˆf(x) = lim
xâ†’+âˆf(x),
0
f(x + t)dG(x) = 0 for all t implies that f = 0 a.s. The property of translation
completeness is shared by many common distribution functions.
Proposition 7.8
Suppose that (X,P) is generated from v and G, and that
(X,P) conforms to Luceâ€™s model with utility index u(x) = ev(x). Suppose that
v : X â†’R is such that v(X) = R, and that G is translation complete. Then there
is Î± > 0 such that G(x) = exp(âˆ’Î± exp(âˆ’x)).
Proof. Fix w âˆˆR and x âˆˆX. Let A = {x,y1,...,yK}, and let Î´i be such that
v(yi) = w + Î´i for i = 1,...,K. Importantly, K is an arbitrary positive integer.
Note that
PA(x) =
exp(v(x))
exp(v(x)) + K
i=1 exp(w + Î´i)
=
,  K
"
i=1
(G(v(x) + t âˆ’w âˆ’Î´i))

dG(t).
Because v(X) = R, we can let all the Î´i â†’0 from below to obtain that (using
the right continuity of G)
exp(v(x))
exp(v(x)) + K exp(w) =
,
(G(v(x) + t âˆ’w))KdG(t).
On the other hand, if we choose z âˆˆX such that v(z) = w + log(K) then
P{x,z}(x) =
0
G(v(x) + t âˆ’w âˆ’log(K))dG(t). Now,
P{x,z}(x) =
exp(v(x))
exp(v(x)) + exp(v(z)) =
exp(v(x))
exp(v(x)) + K exp(w),
hence,
(G(v(x) + t âˆ’w))KdG(t) =
,
(G(v(x) + t âˆ’w âˆ’log(K)))dG(t).
Since, w and x were arbitrary, we obtain that for all w,
, 
(G(v(x) + t âˆ’w))K âˆ’G(v(x) + t âˆ’w âˆ’log(K))dG(t)

= 0.
Since x was arbitrary, we can choose v(x) = 0 and note that by translation
completeness, (G(t))K = G(t âˆ’log(K)) for all t. We claim that there is Î± > 0
such that G(t) = exp(âˆ’Î± exp(âˆ’t)).
First, note that G(t âˆ’log(K)) = (G(t))K implies, setting t = 0, that
G(âˆ’log(K)) = (G(0))K for any positive integer K. Likewise, for any positive
integer L, we have, by setting t = log(K/L), G(âˆ’log(L)) = (G(log(K/L)))K.
But G(âˆ’log(L)) = (G(0))L, so (G(0))L = (G(log(K/L)))K, or G(log(K/L)) =
(G(0))
L
K . G â—¦log is therefore continuous on the strictly positive rational
numbers, and by deï¬nition, it is right-continuous, so that for any strictly
positive real number r, G(log(r)) = (G(0))1/r. Now, let x âˆˆR. We have


--- Page 15 ---
7.3 Random expected utility
109
G(x) = G(log(exp(x))) = (G(0))exp(âˆ’x). Finally, since G(x) is a cumulative
distribution function, we infer that 0 < G(0) < 1, so we may set Î± =
âˆ’log(G(0)) > 0, and obtain G(x) = exp(âˆ’Î± exp(âˆ’x)).
7.3
RANDOM EXPECTED UTILITY
The previous discussion has not sought to limit the structure of rationalizing
preferences in any way. We shall now study the random choice of lotteries, and
consider only von Neumannâ€“Morgenstern expected utility preferences.
Let Y be a ï¬nite set of â€œprizes.â€ The objects of choice will be lotteries over
Y. A lottery is a probability distribution over Y. Let X be the set of all lotteries
over Y. An alternative x in X indicates the probability xi of the ith element
of Y.
In the present setting, a (von Neumannâ€“Morgenstern) utility function is a
vector u âˆˆRY. An expected utility maximizing agent with utility function u
weakly prefers a lottery x over y iff u Â· x â‰¥u Â· y.
We have as before a system of choice probabilities, but where X is inï¬nite
and we deï¬ne the choice only over ï¬nite nonempty sets. So a system of choice
probabilities is a pair (X,P), where PA is a probability distribution over A, for
all ï¬nite nonempty sets A. More speciï¬cally, PA is a Borel probability measure
on X with PA(A) = 1.
We say that (X,P) is expected-utility rationalizable if there is a probability
measure Î¼ on RY such that for every ï¬nite nonempty A and every x âˆˆA,
PA(x) = Î¼

{u âˆˆRY : u Â· x â‰¥u Â· y for all y âˆˆA}

.
The domain of Î¼ is required to be the Ïƒ-algebra generated by all sets of the
form {u âˆˆRY : u Â· x â‰¥u Â· y for all y âˆˆA}. Such sets are discussed in a bit more
detail below. Further, Î¼ is required to be regular, in the sense that for every
possible A, with probability 1, u has a unique maximizer.
The system (X,P) is monotonic if A âŠ†Aâ€² and x âˆˆA, then PA(x) â‰¥PAâ€²(x).
The property of monotonicity, or regularity, was discussed earlier in 7.1. By
Observation 7.1, any rationalizable system of choice probabilities must be
monotonic.
We say that the system (X,P) is linear if
PÎ»A+(1âˆ’Î»)y(Î»x + (1 âˆ’Î»)y) = PA(x).
The notion of linearity is analogous to the property of independence in
expected-utility theory.5
The convex hull of a ï¬nite set A, denoted by conv(A) is the set of all convex
combinations of the elements in A. Write ext(A) for the extreme points of the
5 If âª°is a preference relation over lotteries, independence says that x âª°y iff Î»x + (1 âˆ’Î»)z âª°
Î»y + (1 âˆ’Î»)z, for all z and Î» âˆˆ(0,1). In the present setup, the probability of selecting x is
unaffected by a mixture with y because any of the intended rationalizing preferences should be
unaffected by the mixture.


--- Page 16 ---
110
Stochastic Choice
convex hull of A: these are the points in the convex hull of A which cannot be
written as a convex combination of other points in the convex hull of A.
A system (X,P) is extreme if PA(ext(A)) = 1.
Finally, say that (X,P) is mixture continuous if PÎ»A+(1âˆ’Î»)Aâ€² is continuous as
a function of Î», for all ï¬nite nonempty sets A and Aâ€².
The next result is due to Gul and Pesendorfer.
Theorem 7.9
A system of choice probabilities is expected-utility rationaliz-
able iff it is monotone, linear, extreme, and mixture continuous.
We proceed to discuss the main ideas in the proof of Theorem 7.9. Let
N(A,x) = {u âˆˆRY : u Â· x â‰¥u Â· y
âˆ€y âˆˆA}.
Thus N(A,x) is the set of utilities rationalizing the choice of x from the set A.
Note that the rationalizability of (X,P) by a probability measure Î¼ means that
PA(x) = Î¼(N(A,x)).
The proof of Theorem 7.9 uses basic ideas from convex analysis in
Euclidean spaces. Part of the difï¬culty in the proof is due to the domain of
P being subsets of the simplex, not more general subsets of a Euclidean space.
So the ï¬rst step is to recast the problem using ï¬nite subsets of Rn as the domain
of P. The way to do that is to assume that Y has n+1 elements, and observe that
the (n + 1)-dimensional simplex is contained in an n-dimensional hyperplane.
This hyperplane is isomorphic to Rn. Then use linearity to extend P to the
domain of all ï¬nite subsets of Rn. We omit the details.
Suppose then that we have deï¬ned PA for ï¬nite sets A âŠ†Rn. Linearity can
be more simply recast in this case as stating that PA(x) = PA+{y}(x + y) for all
x âˆˆA.
For P to be rationalizable, we need a probability measure Î¼ for which
PA(x) = Î¼(N(A,x)). So one can deï¬ne a function Î¼ on all sets of the form
N(A,x) by setting Î¼(N(A,x)) = PA(x). The proof proceeds by ï¬rst showing
that Î¼ is well deï¬ned, and then that Î¼ is additive on the family of sets of the
form N(A,x).
Note that if u,v âˆˆN(A,x) then Î±u+Î²v âˆˆN(A,x) for any Î±,Î² â‰¥0. So N(A,x)
is a convex cone. In fact, 0 âˆˆN(A,x) so it is a pointed cone, and A is ï¬nite so
it is a polyhedral cone (one deï¬ned by the intersection of a ï¬nite number of
halfspaces).
To see that Î¼ is well deï¬ned on sets of this kind (pointed polyhedral cones),
we need to establish two things. The ï¬rst is a basic result from convex analysis:
if K is any pointed cone, then there is always a ï¬nite set A and x âˆˆA such that
K = N(A,x). The result is geometrically intuitive, and illustrated in Figure 7.1.
We do not provide a formal proof of the existence of A and x with K = N(A,x),
but hope that the ï¬gure suggests one. The dotted lines in the ï¬gure are obtained
as perpendicular to the extreme rays of the cone.
The second fact we need to establish says that if K = N(A,x) = N(Aâ€²,xâ€²),
then PA(x) = PAâ€²(xâ€²). To prove this fact, note ï¬rst that linearity implies that
PA(x) = PAâˆ’{x}(0). In an abuse of notation, let A and Aâ€² denote A âˆ’{x} and


--- Page 17 ---
7.3 Random expected utility
111
Îš
x
A pointed cone Îš.
(a)
(b)
A set A = (x, xâ€², xâ€²â€²)
such that Îš  = N (A, x).
x
xâ€²
xâ€²â€²
Fig. 7.1 Cones and decision problems.
Îšâ€²
Îš = Îšâ€² âˆª Îšâ€²â€².
(a)
(b)
Îš = Îšâ€² âˆª Îšâ€²â€².
Îšâ€²
x
AÎ»
yÎ»
zÎ»
Îšâ€²â€²
Îšâ€²â€²
Fig. 7.2 Finite additivity.
Aâ€² âˆ’{xâ€²}. Let K = N(A,0) = N(Aâ€²,0). Then we know that pos(A) = N(K,0)
and pos(Aâ€²) = N(K,0), so Aâ€² âŠ†pos(A).6 Since Aâ€² is ï¬nite and 0 âˆˆA, there is
Î» > 0 such that Î»Aâ€² âŠ†conv(A). Then monotonicity and linearity imply that
PAâ€²(0) = PÎ»Aâ€²(0) â‰¥PA(0).
The reverse argument establishes PAâ€²(0) â‰¤PA(0).
To show that Î¼ is (ï¬nitely) additive we need to show that if K, Kâ€², and
Kâ€²â€² are pointed polyhedral cones (meaning sets of the form N(A,0)) such that
K = Kâ€²âˆªKâ€²â€², and the cones have disjoint interior, then Î¼(K) = Î¼(Kâ€²)+Î¼(Kâ€²â€²).7
We give a sketch of the argument based on Figure 7.2. On the left of the ï¬gure
we have a cone K that is the union of Kâ€² and Kâ€²â€², two cones with disjoint
interior. By the previous graphical argument, one can ï¬nd Aâ€² and Aâ€²â€² such that
6 The notation posA stands for the set of all positive linear combinations of elements in A.
7 The intersection of these cones must have probability zero, if we are to obtain a regular
probability measure.


--- Page 18 ---
112
Stochastic Choice
Kâ€² = N(y,Aâ€²) and Kâ€²â€² = N(z,Aâ€²â€²) for some points y and z. Consider x chosen on
the line connecting y and z. Let A be such that K = Kâ€² âˆªKâ€²â€² = N(x,A).
Let AÎ» = (1âˆ’2Î»)A+Î»Aâ€² +Î»Aâ€²â€², and let yÎ» and zÎ» be as in Figure 7.2(a). By
linearity, PAÎ»(yÎ») = PAâ€²(y) = Î¼(Kâ€²) and PAÎ»(zÎ») = PAâ€²â€²(z) = Î¼(Kâ€²â€²).
Let B be any closed ball such that the intersection of B with the extreme
points of A is {x}. Then by choosing Î» small enough we can guarantee that the
only extreme points of AÎ» in B are yÎ» and zÎ». Hence the extremeness property
of P implies that
PAÎ»(B) = PAÎ»(yÎ») + PAÎ»(zÎ») = Î¼(Kâ€²) + Î¼(Kâ€²â€²).
By mixture continuity, PA(B) = limÎ»â†’0 PAÎ»(B). Since B was arbitrary, Î¼(K) =
PA(x) = Î¼(Kâ€²) + Î¼(Kâ€²â€²).
The general argument for additivity is more sophisticated. It relies on
basic results from convex analysis and the idea that N(
i Î²iDi,
i Î²iyi) is
independent of Î²i so long as they are positive (think of the set of all rectangles
with sides parallel to the axes).
Once Î¼ has been established to be an additive measure of the set of all
pointed cones, an extension argument is required to show that it is a probability
measure on vectors u (utility indices).
7.4
CHAPTER REFERENCES
The two interpretations of random choice outlined at the beginning of
the chapter are very standard. Many econometric applications of these
models assume a population distribution of preferences (so-called individual
unobserved heterogeneity). There is also substantial evidence that agents
choose different alternatives when faced with the same choice problem: see,
for example, Mosteller and Nogee (1951), Papandreou (1953), and Chipman
(1960) for early discussions of this fact. Agranov and Ortoleva (2013) conduct
an experiment in which subjects seem to deliberately randomize.
Theorem 7.2 is due to McFadden and Richter (1971, 1990) and Falmagne
(1978). In particular, the equivalence between (I) and (II) is due to McFadden
and Richter, while the equivalence between (I) and (III) is due to Falmagne.
The earlier work of Block and Marschak (1960) had established that
the non-negativity of the Block-Marschak polynomials was necessary for
rationalization. Lemma 7.4 is from Falmagne (1978) (see his Theorem 3).
Proposition 7.3 appears in BarberÂ´a and Pattanaik (1986). The idea of applying
MÂ¨obius inversion to these problems ï¬rst appears in Colonius (1984), see also
Fiorini (2004) and Billot and Thisse (2005). Rota (1964) provides a classic
discussion of MÂ¨obius inversion and the inclusionâ€“exclusion principle.
Luceâ€™s model is presented in Luce (1959). The example of the buses and the
car is due to Debreu (1960b), while a related example attributed to Savage
is presented in Luce and Suppes (1965). This example motivates a model
in which objects are ï¬rst categorized before applying a Luce-style model,


--- Page 19 ---
7.4 Chapter references
113
see Gul, Natenzon, and Pesendorfer (2014). There is plenty of experimental
evidence where subjects exhibit violations of LIIA. One of the best-known
such experiments is reported in Huber, Payne, and Puto (1982): it is called the
attraction effect.
Theorem 7.6 is due to Block and Marschak (1960). The proof presented here
follows Debreu (1960a). The example showing that there are rationalizable
systems that do not conform to Luceâ€™s model is attributed by Block and
Marschak (1960) to Paul Halmos.
Propositions 7.7 and 7.8 appear in McFadden (1974). He attributes
Proposition 7.7 to Holman and Marley, who never published their result.
Proposition 7.8 is part of a collection of uniqueness results regarding models
of random utility: see Yellott (1977). Hausman and Wise (1978) investigate the
related random utility model where error terms are normally distributed.
Theorem 7.9 is due to Gul and Pesendorfer (2006). Our informal discussion
of the proof is largely taken from their paper.
There are other natural approaches to stochastic choice which we have
not discussed here. For example, Machina (1985) suggests that the choice
probabilities attributed to any set may be generated by maximizing a convex
preference relation on the space of lotteries over that set.
